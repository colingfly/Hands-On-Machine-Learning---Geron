{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Is this notebook running on Colab or Kaggle?\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "\n",
    "if IS_COLAB or IS_KAGGLE:\n",
    "    %pip install -q -U tfx\n",
    "    print(\"You can safely ignore the package incompatibility errors.\")\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd \n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:07:51.934291: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "#The Data API\n",
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:07:51.984854: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "#Chaining Transformations\n",
    "dataset = dataset.repeat(3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
      "tf.Tensor([16 18], shape=(2,), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:07:52.155592: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x: x * 2)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:07:52.263046: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.apply(tf.data.experimental.unbatch())\n",
    "\n",
    "dataset = dataset.filter(lambda x: x < 10)\n",
    "for item in dataset.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 6 5 7 3 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 2 1 0 4 6 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([7 2 5 9 2 1 3], shape=(7,), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:07:52.307027: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "#Shuffling Data\n",
    "dataset = tf.data.Dataset.range(10).repeat(3)\n",
    "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n",
    "for item in dataset.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    housing_dir = os.path.join(\"datasets\", \"housing\")\n",
    "    os.makedirs(housing_dir, exist_ok=True)\n",
    "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
    "\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(part_csv)\n",
    "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
    "header = \",\".join(header_cols)\n",
    "\n",
    "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n",
    "valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
    "test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'datasets/housing/my_train_15.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_08.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_03.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_10.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_05.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_19.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_16.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_02.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_09.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_00.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_07.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_12.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_04.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_17.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_11.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_14.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_18.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_06.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_13.csv', shape=(), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:07:52.834192: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)\n",
    "\n",
    "for filepath in filepath_dataset:\n",
    "    print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'4.6477,38.0,5.03728813559322,0.911864406779661,745.0,2.5254237288135593,32.64,-117.07,1.504'\n",
      "b'8.72,44.0,6.163179916317992,1.0460251046025104,668.0,2.794979079497908,34.2,-118.18,4.159'\n",
      "b'3.8456,35.0,5.461346633416459,0.9576059850374065,1154.0,2.8778054862842892,37.96,-122.05,1.598'\n",
      "b'3.3456,37.0,4.514084507042254,0.9084507042253521,458.0,3.2253521126760565,36.67,-121.7,2.526'\n",
      "b'3.6875,44.0,4.524475524475524,0.993006993006993,457.0,3.195804195804196,34.04,-118.15,1.625'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:07:52.942122: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers)\n",
    "\n",
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_defaults=[0, np.nan, tf.constant(np.nan, dtype=tf.float64), \"Hello\", tf.constant([])]\n",
    "parsed_fields = tf.io.decode_csv('1,2,3,4,5', record_defaults)\n",
    "\n",
    "parsed_fields = tf.io.decode_csv(',,,,5', record_defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{function_node __wrapped__DecodeCSV_OUT_TYPE_5_device_/job:localhost/replica:0/task:0/device:CPU:0}} Field 4 is required but missing in record 0! [Op:DecodeCSV]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:07:52.978460: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: Field 4 is required but missing in record 0!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    parsed_fields = tf.io.decode_csv(',,,,', record_defaults)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{function_node __wrapped__DecodeCSV_OUT_TYPE_5_device_/job:localhost/replica:0/task:0/device:CPU:0}} Expect 5 fields but have 7 in record 0 [Op:DecodeCSV]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:07:53.006463: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: Expect 5 fields but have 7 in record 0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    parsed_fields = tf.io.decode_csv('1,2,3,4,5,6,7', record_defaults)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 8 # X_train.shape[-1]\n",
    "\n",
    "def preprocess(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return (x - X_mean) / X_std, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([ 0.16579159,  1.216324  , -0.05204564, -0.39215982, -0.5277444 ,\n",
       "        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)"
      ]
     },
     "execution_count": 820,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([ 0.16579159,  1.216324  , -0.05204564, -0.39215982, -0.5277444 ,\n",
       "        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)"
      ]
     },
     "execution_count": 821,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def csv_reader_dataset(filepaths, repeat=1, n_readers=5,\n",
    "                       n_read_threads=None, shuffle_buffer_size=10000,\n",
    "                       n_parse_threads=5, batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat)\n",
    "    return dataset.batch(batch_size).prefetch(1)\n",
    "\n",
    "\n",
    "preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = tf.Tensor(\n",
      "[[ 0.5804519  -0.20762321  0.05616303 -0.15191229  0.01343246  0.00604472\n",
      "   1.2525111  -1.3671792 ]\n",
      " [ 5.818099    1.8491895   1.1784915   0.28173092 -1.2496178  -0.3571987\n",
      "   0.7231292  -1.0023477 ]\n",
      " [-0.9253566   0.5834586  -0.7807257  -0.28213993 -0.36530012  0.27389365\n",
      "  -0.76194876  0.72684526]], shape=(3, 8), dtype=float32)\n",
      "y = tf.Tensor(\n",
      "[[1.752]\n",
      " [1.313]\n",
      " [1.535]], shape=(3, 1), dtype=float32)\n",
      "\n",
      "X = tf.Tensor(\n",
      "[[-0.8324941   0.6625668  -0.20741376 -0.18699841 -0.14536144  0.09635526\n",
      "   0.9807942  -0.67250353]\n",
      " [-0.62183803  0.5834586  -0.19862501 -0.3500319  -1.1437552  -0.3363751\n",
      "   1.107282   -0.8674123 ]\n",
      " [ 0.8683102   0.02970133  0.3427381  -0.29872298  0.7124906   0.28026953\n",
      "  -0.72915536  0.86178064]], shape=(3, 8), dtype=float32)\n",
      "y = tf.Tensor(\n",
      "[[0.919]\n",
      " [1.028]\n",
      " [2.182]], shape=(3, 1), dtype=float32)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:07:53.768843: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "train_set = csv_reader_dataset(train_filepaths, batch_size=3)\n",
    "for X_batch, y_batch in train_set.take(2):\n",
    "    print(\"X =\", X_batch)\n",
    "    print(\"y =\", y_batch)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = csv_reader_dataset(train_filepaths)\n",
    "valid_set = csv_reader_dataset(valid_filepaths)\n",
    "test_set = csv_reader_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.0673 - val_loss: 1.4326\n",
      "Epoch 2/10\n",
      "\u001b[1m  1/362\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1894"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:07:57.427194: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "2024-11-12 16:07:57.446299: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 624us/step - loss: 1.1894 - val_loss: 1.4298\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:07:57.660748: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8987 - val_loss: 0.7631\n",
      "Epoch 4/10\n",
      "\u001b[1m  1/362\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0656"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:07:58.416198: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2024-11-12 16:07:58.430473: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 619us/step - loss: 1.0656 - val_loss: 0.7641\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:07:58.644059: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.7752 - val_loss: 0.9528\n",
      "Epoch 6/10\n",
      "\u001b[1m  1/362\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4286"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:07:59.549647: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2024-11-12 16:07:59.563431: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - loss: 0.4286 - val_loss: 0.9562\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:07:59.781924: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.7340 - val_loss: 0.6786\n",
      "Epoch 8/10\n",
      "\u001b[1m  1/362\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4833"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:08:00.593999: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2024-11-12 16:08:00.608471: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 631us/step - loss: 0.4833 - val_loss: 0.6773\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:08:00.824082: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.7047 - val_loss: 1.2406\n",
      "Epoch 10/10\n",
      "\u001b[1m  1/362\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2532"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:08:01.681740: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2024-11-12 16:08:01.696507: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 570us/step - loss: 1.2532 - val_loss: 1.2401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:08:01.891369: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x188bd0810>"
      ]
     },
     "execution_count": 824,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "batch_size = 32\n",
    "model.fit(train_set, steps_per_epoch=len(X_train) // batch_size, epochs=10,\n",
    "          validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 0.6738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.659783124923706"
      ]
     },
     "execution_count": 825,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set, steps=len(X_test) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.127331 ],\n",
       "       [1.8713481],\n",
       "       [1.2954732],\n",
       "       ...,\n",
       "       [3.0071936],\n",
       "       [1.4556596],\n",
       "       [3.056291 ]], dtype=float32)"
      ]
     },
     "execution_count": 826,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_set = test_set.map(lambda X, y: X) # we could instead just pass test_set, Keras would ignore the labels\n",
    "X_new = X_test\n",
    "model.predict(new_set, steps=len(X_new) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step 363/1810"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:08:16.486434: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = keras.losses.MeanSquaredError()\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps_per_epoch = len(X_train) // batch_size\n",
    "total_steps = n_epochs * n_steps_per_epoch\n",
    "global_step = 0\n",
    "for X_batch, y_batch in train_set.take(total_steps):\n",
    "    global_step += 1\n",
    "    print(\"\\rGlobal step {}/{}\".format(global_step, total_steps), end=\"\")\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X_batch)\n",
    "        main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "        loss = tf.add_n([main_loss] + model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = keras.losses.MeanSquaredError()\n",
    "\n",
    "@tf.function\n",
    "def train(model, n_epochs, batch_size=32,\n",
    "          n_readers=5, n_read_threads=5, shuffle_buffer_size=10000, n_parse_threads=5):\n",
    "    train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, n_readers=n_readers,\n",
    "                       n_read_threads=n_read_threads, shuffle_buffer_size=shuffle_buffer_size,\n",
    "                       n_parse_threads=n_parse_threads, batch_size=batch_size)\n",
    "    for X_batch, y_batch in train_set:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "train(model, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step 100 / 1810\n",
      "Global step 200 / 1810\n",
      "Global step 300 / 1810\n",
      "Global step 400 / 1810\n",
      "Global step 500 / 1810\n",
      "Global step 600 / 1810\n",
      "Global step 700 / 1810\n",
      "Global step 800 / 1810\n",
      "Global step 900 / 1810\n",
      "Global step 1000 / 1810\n",
      "Global step 1100 / 1810\n",
      "Global step 1200 / 1810\n",
      "Global step 1300 / 1810\n",
      "Global step 1400 / 1810\n",
      "Global step 1500 / 1810\n",
      "Global step 1600 / 1810\n",
      "Global step 1700 / 1810\n",
      "Global step 1800 / 1810\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = keras.losses.MeanSquaredError()\n",
    "\n",
    "@tf.function\n",
    "def train(model, n_epochs, batch_size=32,\n",
    "          n_readers=5, n_read_threads=5, shuffle_buffer_size=10000, n_parse_threads=5):\n",
    "    train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, n_readers=n_readers,\n",
    "                       n_read_threads=n_read_threads, shuffle_buffer_size=shuffle_buffer_size,\n",
    "                       n_parse_threads=n_parse_threads, batch_size=batch_size)\n",
    "    n_steps_per_epoch = len(X_train) // batch_size\n",
    "    total_steps = n_epochs * n_steps_per_epoch\n",
    "    global_step = 0\n",
    "    for X_batch, y_batch in train_set.take(total_steps):\n",
    "        global_step += 1\n",
    "        if tf.equal(global_step % 100, 0):\n",
    "            tf.print(\"\\rGlobal step\", global_step, \"/\", total_steps)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "train(model, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "● apply()              Applies a transformation function to this dataset.\n",
      "● as_numpy_iterator()  Returns an iterator which converts all elements of the dataset to numpy.\n",
      "● batch()              Combines consecutive elements of this dataset into batches.\n",
      "● bucket_by_sequence_length()A transformation that buckets elements in a `Dataset` by length.\n",
      "● cache()              Caches the elements in this dataset.\n",
      "● cardinality()        Returns the cardinality of the dataset, if known.\n",
      "● choose_from_datasets()Creates a dataset that deterministically chooses elements from `datasets`.\n",
      "● concatenate()        Creates a `Dataset` by concatenating the given dataset with this dataset.\n",
      "● counter()            Creates a `Dataset` that counts from `start` in steps of size `step`.\n",
      "● element_spec()       The type specification of an element of this dataset.\n",
      "● enumerate()          Enumerates the elements of this dataset.\n",
      "● filter()             Filters this dataset according to `predicate`.\n",
      "● fingerprint()        Computes the fingerprint of this `Dataset`.\n",
      "● flat_map()           Maps `map_func` across this dataset and flattens the result.\n",
      "● from_generator()     Creates a `Dataset` whose elements are generated by `generator`. (deprecated arguments)\n",
      "● from_tensor_slices() Creates a `Dataset` whose elements are slices of the given tensors.\n",
      "● from_tensors()       Creates a `Dataset` with a single element, comprising the given tensors.\n",
      "● get_single_element() Returns the single element of the `dataset`.\n",
      "● group_by_window()    Groups windows of elements by key and reduces them.\n",
      "● ignore_errors()      Drops elements that cause errors.\n",
      "● interleave()         Maps `map_func` across this dataset, and interleaves the results.\n",
      "● list_files()         A dataset of all files matching one or more glob patterns.\n",
      "● load()               Loads a previously saved dataset.\n",
      "● map()                Maps `map_func` across the elements of this dataset.\n",
      "● options()            Returns the options for this dataset and its inputs.\n",
      "● padded_batch()       Combines consecutive elements of this dataset into padded batches.\n",
      "● prefetch()           Creates a `Dataset` that prefetches elements from this dataset.\n",
      "● ragged_batch()       Combines consecutive elements of this dataset into `tf.RaggedTensor`s.\n",
      "● random()             Creates a `Dataset` of pseudorandom values.\n",
      "● range()              Creates a `Dataset` of a step-separated range of values.\n",
      "● rebatch()            Creates a `Dataset` that rebatches the elements from this dataset.\n",
      "● reduce()             Reduces the input dataset to a single element.\n",
      "● rejection_resample() Resamples elements to reach a target distribution.\n",
      "● repeat()             Repeats this dataset so each original value is seen `count` times.\n",
      "● sample_from_datasets()Samples elements at random from the datasets in `datasets`.\n",
      "● save()               Saves the content of the given dataset.\n",
      "● scan()               A transformation that scans a function across an input dataset.\n",
      "● shard()              Creates a `Dataset` that includes only 1/`num_shards` of this dataset.\n",
      "● shuffle()            Randomly shuffles the elements of this dataset.\n",
      "● skip()               Creates a `Dataset` that skips `count` elements from this dataset.\n",
      "● snapshot()           API to persist the output of the input dataset.\n",
      "● sparse_batch()       Combines consecutive elements into `tf.sparse.SparseTensor`s.\n",
      "● take()               Creates a `Dataset` with at most `count` elements from this dataset.\n",
      "● take_while()         A transformation that stops dataset iteration based on a `predicate`.\n",
      "● unbatch()            Splits elements of a dataset into multiple elements.\n",
      "● unique()             A transformation that discards duplicate elements of a `Dataset`.\n",
      "● window()             Returns a dataset of \"windows\".\n",
      "● with_options()       Returns a new `tf.data.Dataset` with the given options set.\n",
      "● zip()                Creates a `Dataset` by zipping together the given datasets.\n"
     ]
    }
   ],
   "source": [
    "for m in dir(tf.data.Dataset):\n",
    "    if not (m.startswith(\"_\") or m.endswith(\"_\")):\n",
    "        func = getattr(tf.data.Dataset, m)\n",
    "        if hasattr(func, \"__doc__\"):\n",
    "            print(\"● {:21s}{}\".format(m + \"()\", func.__doc__.split(\"\\n\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'First Record', shape=(), dtype=string)\n",
      "tf.Tensor(b'Second Record', shape=(), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:08:25.825651: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "#TFRecord\n",
    "with tf.io.TFRecordWriter('my_data.tfrecord') as f:\n",
    "    f.write(b\"First Record\")\n",
    "    f.write(b\"Second Record\")\n",
    "\n",
    "filepaths = ['my_data.tfrecord']\n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'First Record', shape=(), dtype=string)\n",
      "tf.Tensor(b'Second Record', shape=(), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:08:25.889169: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "options = tf.io.TFRecordOptions(compression_type='GZIP')\n",
    "with tf.io.TFRecordWriter('my_compressed.tfrecord', options) as f:\n",
    "    f.write(b\"First Record\")\n",
    "    f.write(b\"Second Record\")\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(['my_compressed.tfrecord'], compression_type='GZIP')\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
      ]
     },
     "execution_count": 838,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing = load_housing_data()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_median_age = tf.feature_column.numeric_column(\"housing_median_age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_mean, age_std = X_mean[1], X_std[1]  # The median age is column in 1\n",
    "housing_median_age = tf.feature_column.numeric_column(\n",
    "    \"housing_median_age\", normalizer_fn=lambda x: (x - age_mean) / age_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_income = tf.feature_column.numeric_column(\"median_income\")\n",
    "bucketized_income = tf.feature_column.bucketized_column(\n",
    "    median_income, boundaries=[1.5, 3., 4.5, 6.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BucketizedColumn(source_column=NumericColumn(key='median_income', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(1.5, 3.0, 4.5, 6.0))"
      ]
     },
     "execution_count": 842,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucketized_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocean_prox_vocab = ['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN']\n",
    "ocean_proximity = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"ocean_proximity\", ocean_prox_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketized_age = tf.feature_column.bucketized_column(\n",
    "    housing_median_age, boundaries=[-1., -0.5, 0., 0.5, 1.]) # age was scaled\n",
    "age_and_ocean_proximity = tf.feature_column.crossed_column(\n",
    "    [bucketized_age, ocean_proximity], hash_bucket_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude = tf.feature_column.numeric_column(\"latitude\")\n",
    "longitude = tf.feature_column.numeric_column(\"longitude\")\n",
    "bucketized_latitude = tf.feature_column.bucketized_column(\n",
    "    latitude, boundaries=list(np.linspace(32., 42., 20 - 1)))\n",
    "bucketized_longitude = tf.feature_column.bucketized_column(\n",
    "    longitude, boundaries=list(np.linspace(-125., -114., 20 - 1)))\n",
    "location = tf.feature_column.crossed_column(\n",
    "    [bucketized_latitude, bucketized_longitude], hash_bucket_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocean_proximity_one_hot = tf.feature_column.indicator_column(ocean_proximity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocean_proximity_embed = tf.feature_column.embedding_column(ocean_proximity,\n",
    "                                                           dimension=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_house_value = tf.feature_column.numeric_column(\"median_house_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'housing_median_age': FixedLenFeature(shape=(1,), dtype=tf.float32, default_value=None),\n",
       " 'median_house_value': FixedLenFeature(shape=(1,), dtype=tf.float32, default_value=None)}"
      ]
     },
     "execution_count": 849,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [housing_median_age, median_house_value]\n",
    "feature_descriptions = tf.feature_column.make_parse_example_spec(columns)\n",
    "feature_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import Example, Features, Feature, FloatList\n",
    "\n",
    "with tf.io.TFRecordWriter(\"my_data_with_features.tfrecords\") as f:\n",
    "    for x, y in zip(X_train[:, 1:2], y_train):\n",
    "        example = Example(features=Features(feature={\n",
    "            \"housing_median_age\": Feature(float_list=FloatList(value=[float(x)])),\n",
    "            \"median_house_value\": Feature(float_list=FloatList(value=[float(y)]))\n",
    "        }))\n",
    "        f.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_examples(serialized_examples):\n",
    "    examples = tf.io.parse_example(serialized_examples, feature_descriptions)\n",
    "    targets = examples.pop(\"median_house_value\") # separate the targets\n",
    "    return examples, targets\n",
    "\n",
    "batch_size = 32\n",
    "dataset = tf.data.TFRecordDataset([\"my_data_with_features.tfrecords\"])\n",
    "dataset = dataset.repeat().shuffle(10000).batch(batch_size).map(parse_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 9.8537e-04 - loss: 27.5433\n",
      "Epoch 2/5\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0021 - loss: 2.0787\n",
      "Epoch 3/5\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 987us/step - accuracy: 0.0032 - loss: 1.4636\n",
      "Epoch 4/5\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - accuracy: 0.0033 - loss: 1.3788\n",
      "Epoch 5/5\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 631us/step - accuracy: 0.0034 - loss: 1.3542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1868f39d0>"
      ]
     },
     "execution_count": 853,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_without_target = columns[:-1]\n",
    "input_shape = len(columns_without_target)  # Assuming each column is a feature\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(units=input_shape, activation=\"relu\", input_shape=(input_shape,)),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(dataset, steps_per_epoch=len(X_train) // batch_size, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets = tfds.load(name=\"mnist\")\n",
    "mnist_train, mnist_test = datasets[\"train\"], datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abstract_reasoning', 'accentdb', 'aeslc', 'aflw2k3d', 'ag_news_subset', 'ai2_arc', 'ai2_arc_with_ir', 'ai2dcaption', 'aloha_mobile', 'amazon_us_reviews', 'anli', 'answer_equivalence', 'arc', 'asqa', 'asset', 'assin2', 'asu_table_top_converted_externally_to_rlds', 'austin_buds_dataset_converted_externally_to_rlds', 'austin_sailor_dataset_converted_externally_to_rlds', 'austin_sirius_dataset_converted_externally_to_rlds', 'bair_robot_pushing_small', 'bc_z', 'bccd', 'beans', 'bee_dataset', 'beir', 'berkeley_autolab_ur5', 'berkeley_cable_routing', 'berkeley_fanuc_manipulation', 'berkeley_gnm_cory_hall', 'berkeley_gnm_recon', 'berkeley_gnm_sac_son', 'berkeley_mvp_converted_externally_to_rlds', 'berkeley_rpt_converted_externally_to_rlds', 'big_patent', 'bigearthnet', 'billsum', 'binarized_mnist', 'binary_alpha_digits', 'ble_wind_field', 'blimp', 'booksum', 'bool_q', 'bot_adversarial_dialogue', 'bridge', 'bridge_data_msr', 'bucc', 'c4', 'c4_wsrs', 'caltech101', 'caltech_birds2010', 'caltech_birds2011', 'cardiotox', 'cars196', 'cassava', 'cats_vs_dogs', 'celeb_a', 'celeb_a_hq', 'cfq', 'cherry_blossoms', 'chexpert', 'cifar10', 'cifar100', 'cifar100_n', 'cifar10_1', 'cifar10_corrupted', 'cifar10_h', 'cifar10_n', 'citrus_leaves', 'cityscapes', 'civil_comments', 'clevr', 'clic', 'clinc_oos', 'cmaterdb', 'cmu_franka_exploration_dataset_converted_externally_to_rlds', 'cmu_play_fusion', 'cmu_stretch', 'cnn_dailymail', 'coco', 'coco_captions', 'coil100', 'colorectal_histology', 'colorectal_histology_large', 'columbia_cairlab_pusht_real', 'common_voice', 'conll2002', 'conll2003', 'conq_hose_manipulation', 'controlled_noisy_web_labels', 'coqa', 'corr2cause', 'cos_e', 'cosmos_qa', 'covid19', 'covid19sum', 'crema_d', 'criteo', 'cs_restaurants', 'curated_breast_imaging_ddsm', 'cycle_gan', 'd4rl_adroit_door', 'd4rl_adroit_hammer', 'd4rl_adroit_pen', 'd4rl_adroit_relocate', 'd4rl_antmaze', 'd4rl_mujoco_ant', 'd4rl_mujoco_halfcheetah', 'd4rl_mujoco_hopper', 'd4rl_mujoco_walker2d', 'dart', 'databricks_dolly', 'davis', 'deep1b', 'deep_weeds', 'definite_pronoun_resolution', 'dementiabank', 'diabetic_retinopathy_detection', 'diamonds', 'dices', 'div2k', 'dlr_edan_shared_control_converted_externally_to_rlds', 'dlr_sara_grid_clamp_converted_externally_to_rlds', 'dlr_sara_pour_converted_externally_to_rlds', 'dmlab', 'dobbe', 'doc_nli', 'dolma', 'dolphin_number_word', 'domainnet', 'downsampled_imagenet', 'drop', 'dsprites', 'dtd', 'duke_ultrasound', 'e2e_cleaned', 'efron_morris75', 'emnist', 'eraser_multi_rc', 'esnli', 'eth_agent_affordances', 'eurosat', 'fashion_mnist', 'flic', 'flores', 'fmb', 'food101', 'forest_fires', 'fractal20220817_data', 'fuss', 'gap', 'geirhos_conflict_stimuli', 'gem', 'genomics_ood', 'german_credit_numeric', 'gigaword', 'glove100_angular', 'glue', 'goemotions', 'gov_report', 'gpt3', 'gref', 'groove', 'grounded_scan', 'gsm8k', 'gtzan', 'gtzan_music_speech', 'hellaswag', 'higgs', 'hillstrom', 'horses_or_humans', 'howell', 'i_naturalist2017', 'i_naturalist2018', 'i_naturalist2021', 'iamlab_cmu_pickup_insert_converted_externally_to_rlds', 'imagenet2012', 'imagenet2012_corrupted', 'imagenet2012_fewshot', 'imagenet2012_multilabel', 'imagenet2012_real', 'imagenet2012_subset', 'imagenet_a', 'imagenet_lt', 'imagenet_pi', 'imagenet_r', 'imagenet_resized', 'imagenet_sketch', 'imagenet_v2', 'imagenette', 'imagewang', 'imdb_reviews', 'imperialcollege_sawyer_wrist_cam', 'io_ai_tech', 'irc_disentanglement', 'iris', 'istella', 'jaco_play', 'kaist_nonprehensile_converted_externally_to_rlds', 'kddcup99', 'kitti', 'kmnist', 'kuka', 'laion400m', 'lambada', 'lfw', 'librispeech', 'librispeech_lm', 'libritts', 'ljspeech', 'lm1b', 'locomotion', 'lost_and_found', 'lsun', 'lvis', 'malaria', 'maniskill_dataset_converted_externally_to_rlds', 'math_dataset', 'math_qa', 'mctaco', 'media_sum', 'mimic_play', 'mlqa', 'mnist', 'mnist_corrupted', 'movie_lens', 'movie_rationales', 'movielens', 'moving_mnist', 'mrqa', 'mslr_web', 'mt_opt', 'mtnt', 'multi_news', 'multi_nli', 'multi_nli_mismatch', 'natural_instructions', 'natural_questions', 'natural_questions_open', 'newsroom', 'nsynth', 'nyu_depth_v2', 'nyu_door_opening_surprising_effectiveness', 'nyu_franka_play_dataset_converted_externally_to_rlds', 'nyu_rot_dataset_converted_externally_to_rlds', 'ogbg_molpcba', 'omniglot', 'open_images_challenge2019_detection', 'open_images_v4', 'openbookqa', 'opinion_abstracts', 'opinosis', 'opus', 'oxford_flowers102', 'oxford_iiit_pet', 'para_crawl', 'pass', 'patch_camelyon', 'paws_wiki', 'paws_x_wiki', 'penguins', 'pet_finder', 'pg19', 'piqa', 'places365_small', 'placesfull', 'plant_leaves', 'plant_village', 'plantae_k', 'plex_robosuite', 'pneumonia_mnist', 'protein_net', 'q_re_cc', 'qa4mre', 'qasc', 'qm9', 'quac', 'quality', 'quickdraw_bitmap', 'race', 'radon', 'real_toxicity_prompts', 'reddit', 'reddit_disentanglement', 'reddit_tifu', 'ref_coco', 'resisc45', 'rlu_atari', 'rlu_atari_checkpoints', 'rlu_atari_checkpoints_ordered', 'rlu_control_suite', 'rlu_dmlab_explore_object_rewards_few', 'rlu_dmlab_explore_object_rewards_many', 'rlu_dmlab_rooms_select_nonmatching_object', 'rlu_dmlab_rooms_watermaze', 'rlu_dmlab_seekavoid_arena01', 'rlu_locomotion', 'rlu_rwrl', 'robo_set', 'robomimic_mg', 'robomimic_mh', 'robomimic_ph', 'robonet', 'robosuite_panda_pick_place_can', 'roboturk', 'rock_paper_scissors', 'rock_you', 's3o4d', 'salient_span_wikipedia', 'samsum', 'savee', 'scan', 'scene_parse150', 'schema_guided_dialogue', 'sci_tail', 'scicite', 'scientific_papers', 'scrolls', 'segment_anything', 'sentiment140', 'shapes3d', 'sift1m', 'simpte', 'siscore', 'smallnorb', 'smart_buildings', 'smartwatch_gestures', 'snli', 'so2sat', 'speech_commands', 'spoc_robot', 'spoken_digit', 'squad', 'squad_question_generation', 'stanford_dogs', 'stanford_hydra_dataset_converted_externally_to_rlds', 'stanford_kuka_multimodal_dataset_converted_externally_to_rlds', 'stanford_mask_vit_converted_externally_to_rlds', 'stanford_online_products', 'stanford_robocook_converted_externally_to_rlds', 'star_cfq', 'starcraft_video', 'stl10', 'story_cloze', 'summscreen', 'sun397', 'super_glue', 'svhn_cropped', 'symmetric_solids', 'taco_play', 'tao', 'tatoeba', 'ted_hrlr_translate', 'ted_multi_translate', 'tedlium', 'tf_flowers', 'the300w_lp', 'tidybot', 'tiny_shakespeare', 'titanic', 'tokyo_u_lsmo_converted_externally_to_rlds', 'toto', 'trec', 'trivia_qa', 'tydi_qa', 'uc_merced', 'ucf101', 'ucsd_kitchen_dataset_converted_externally_to_rlds', 'ucsd_pick_and_place_dataset_converted_externally_to_rlds', 'uiuc_d3field', 'unified_qa', 'universal_dependencies', 'unnatural_instructions', 'usc_cloth_sim_converted_externally_to_rlds', 'user_libri_audio', 'user_libri_text', 'utaustin_mutex', 'utokyo_pr2_opening_fridge_converted_externally_to_rlds', 'utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds', 'utokyo_saytap_converted_externally_to_rlds', 'utokyo_xarm_bimanual_converted_externally_to_rlds', 'utokyo_xarm_pick_and_place_converted_externally_to_rlds', 'vctk', 'vima_converted_externally_to_rlds', 'viola', 'visual_domain_decathlon', 'voc', 'voxceleb', 'voxforge', 'wake_vision', 'waymo_open_dataset', 'web_graph', 'web_nlg', 'web_questions', 'webvid', 'wider_face', 'wiki40b', 'wiki_auto', 'wiki_bio', 'wiki_dialog', 'wiki_table_questions', 'wiki_table_text', 'wikiann', 'wikihow', 'wikipedia', 'wikipedia_toxicity_subtypes', 'wine_quality', 'winogrande', 'wit', 'wit_kaggle', 'wmt13_translate', 'wmt14_translate', 'wmt15_translate', 'wmt16_translate', 'wmt17_translate', 'wmt18_translate', 'wmt19_translate', 'wmt_t2t_translate', 'wmt_translate', 'wordnet', 'wsc273', 'xnli', 'xquad', 'xsum', 'xtreme_pawsx', 'xtreme_pos', 'xtreme_s', 'xtreme_xnli', 'yahoo_ltrc', 'yelp_polarity_reviews', 'yes_no', 'youtube_vis', 'huggingface:acronym_identification', 'huggingface:ade_corpus_v2', 'huggingface:adv_glue', 'huggingface:adversarial_qa', 'huggingface:aeslc', 'huggingface:afrikaans_ner_corpus', 'huggingface:ag_news', 'huggingface:ai2_arc', 'huggingface:air_dialogue', 'huggingface:ajgt_twitter_ar', 'huggingface:allegro_reviews', 'huggingface:allocine', 'huggingface:alt', 'huggingface:amazon_polarity', 'huggingface:amazon_reviews_multi', 'huggingface:amazon_us_reviews', 'huggingface:ambig_qa', 'huggingface:americas_nli', 'huggingface:ami', 'huggingface:amttl', 'huggingface:anli', 'huggingface:app_reviews', 'huggingface:aqua_rat', 'huggingface:aquamuse', 'huggingface:ar_cov19', 'huggingface:ar_res_reviews', 'huggingface:ar_sarcasm', 'huggingface:arabic_billion_words', 'huggingface:arabic_pos_dialect', 'huggingface:arabic_speech_corpus', 'huggingface:arcd', 'huggingface:arsentd_lev', 'huggingface:art', 'huggingface:arxiv_dataset', 'huggingface:ascent_kb', 'huggingface:aslg_pc12', 'huggingface:asnq', 'huggingface:asset', 'huggingface:assin', 'huggingface:assin2', 'huggingface:atomic', 'huggingface:autshumato', 'huggingface:babi_qa', 'huggingface:banking77', 'huggingface:bbaw_egyptian', 'huggingface:bbc_hindi_nli', 'huggingface:bc2gm_corpus', 'huggingface:beans', 'huggingface:best2009', 'huggingface:bianet', 'huggingface:bible_para', 'huggingface:big_patent', 'huggingface:bigbench', 'huggingface:billsum', 'huggingface:bing_coronavirus_query_set', 'huggingface:biomrc', 'huggingface:biosses', 'huggingface:biwi_kinect_head_pose', 'huggingface:blbooks', 'huggingface:blbooksgenre', 'huggingface:blended_skill_talk', 'huggingface:blimp', 'huggingface:blog_authorship_corpus', 'huggingface:bn_hate_speech', 'huggingface:bnl_newspapers', 'huggingface:bookcorpus', 'huggingface:bookcorpusopen', 'huggingface:boolq', 'huggingface:bprec', 'huggingface:break_data', 'huggingface:brwac', 'huggingface:bsd_ja_en', 'huggingface:bswac', 'huggingface:c3', 'huggingface:c4', 'huggingface:cail2018', 'huggingface:caner', 'huggingface:capes', 'huggingface:casino', 'huggingface:catalonia_independence', 'huggingface:cats_vs_dogs', 'huggingface:cawac', 'huggingface:cbt', 'huggingface:cc100', 'huggingface:cc_news', 'huggingface:ccaligned_multilingual', 'huggingface:cdsc', 'huggingface:cdt', 'huggingface:cedr', 'huggingface:cfq', 'huggingface:chr_en', 'huggingface:cifar10', 'huggingface:cifar100', 'huggingface:circa', 'huggingface:civil_comments', 'huggingface:clickbait_news_bg', 'huggingface:climate_fever', 'huggingface:clinc_oos', 'huggingface:clue', 'huggingface:cmrc2018', 'huggingface:cmu_hinglish_dog', 'huggingface:cnn_dailymail', 'huggingface:coached_conv_pref', 'huggingface:coarse_discourse', 'huggingface:codah', 'huggingface:code_search_net', 'huggingface:code_x_glue_cc_clone_detection_big_clone_bench', 'huggingface:code_x_glue_cc_clone_detection_poj104', 'huggingface:code_x_glue_cc_cloze_testing_all', 'huggingface:code_x_glue_cc_cloze_testing_maxmin', 'huggingface:code_x_glue_cc_code_completion_line', 'huggingface:code_x_glue_cc_code_completion_token', 'huggingface:code_x_glue_cc_code_refinement', 'huggingface:code_x_glue_cc_code_to_code_trans', 'huggingface:code_x_glue_cc_defect_detection', 'huggingface:code_x_glue_ct_code_to_text', 'huggingface:code_x_glue_tc_nl_code_search_adv', 'huggingface:code_x_glue_tc_text_to_code', 'huggingface:code_x_glue_tt_text_to_text', 'huggingface:com_qa', 'huggingface:common_gen', 'huggingface:common_language', 'huggingface:common_voice', 'huggingface:commonsense_qa', 'huggingface:competition_math', 'huggingface:compguesswhat', 'huggingface:conceptnet5', 'huggingface:conceptual_12m', 'huggingface:conceptual_captions', 'huggingface:conll2000', 'huggingface:conll2002', 'huggingface:conll2003', 'huggingface:conll2012_ontonotesv5', 'huggingface:conllpp', 'huggingface:consumer-finance-complaints', 'huggingface:conv_ai', 'huggingface:conv_ai_2', 'huggingface:conv_ai_3', 'huggingface:conv_questions', 'huggingface:coqa', 'huggingface:cord19', 'huggingface:cornell_movie_dialog', 'huggingface:cos_e', 'huggingface:cosmos_qa', 'huggingface:counter', 'huggingface:covid_qa_castorini', 'huggingface:covid_qa_deepset', 'huggingface:covid_qa_ucsd', 'huggingface:covid_tweets_japanese', 'huggingface:covost2', 'huggingface:cppe-5', 'huggingface:craigslist_bargains', 'huggingface:crawl_domain', 'huggingface:crd3', 'huggingface:crime_and_punish', 'huggingface:crows_pairs', 'huggingface:cryptonite', 'huggingface:cs_restaurants', 'huggingface:cuad', 'huggingface:curiosity_dialogs', 'huggingface:daily_dialog', 'huggingface:dane', 'huggingface:danish_political_comments', 'huggingface:dart', 'huggingface:datacommons_factcheck', 'huggingface:dbpedia_14', 'huggingface:dbrd', 'huggingface:deal_or_no_dialog', 'huggingface:definite_pronoun_resolution', 'huggingface:dengue_filipino', 'huggingface:dialog_re', 'huggingface:diplomacy_detection', 'huggingface:disaster_response_messages', 'huggingface:discofuse', 'huggingface:discovery', 'huggingface:disfl_qa', 'huggingface:doc2dial', 'huggingface:docred', 'huggingface:doqa', 'huggingface:dream', 'huggingface:drop', 'huggingface:duorc', 'huggingface:dutch_social', 'huggingface:dyk', 'huggingface:e2e_nlg', 'huggingface:e2e_nlg_cleaned', 'huggingface:ecb', 'huggingface:ecthr_cases', 'huggingface:eduge', 'huggingface:ehealth_kd', 'huggingface:eitb_parcc', 'huggingface:electricity_load_diagrams', 'huggingface:eli5', 'huggingface:eli5_category', 'huggingface:elkarhizketak', 'huggingface:emea', 'huggingface:emo', 'huggingface:emotion', 'huggingface:emotone_ar', 'huggingface:empathetic_dialogues', 'huggingface:enriched_web_nlg', 'huggingface:enwik8', 'huggingface:eraser_multi_rc', 'huggingface:esnli', 'huggingface:eth_py150_open', 'huggingface:ethos', 'huggingface:ett', 'huggingface:eu_regulatory_ir', 'huggingface:eurlex', 'huggingface:euronews', 'huggingface:europa_eac_tm', 'huggingface:europa_ecdc_tm', 'huggingface:europarl_bilingual', 'huggingface:event2Mind', 'huggingface:evidence_infer_treatment', 'huggingface:exams', 'huggingface:factckbr', 'huggingface:fake_news_english', 'huggingface:fake_news_filipino', 'huggingface:farsi_news', 'huggingface:fashion_mnist', 'huggingface:fever', 'huggingface:few_rel', 'huggingface:financial_phrasebank', 'huggingface:finer', 'huggingface:flores', 'huggingface:flue', 'huggingface:food101', 'huggingface:fquad', 'huggingface:freebase_qa', 'huggingface:gap', 'huggingface:gem', 'huggingface:generated_reviews_enth', 'huggingface:generics_kb', 'huggingface:german_legal_entity_recognition', 'huggingface:germaner', 'huggingface:germeval_14', 'huggingface:giga_fren', 'huggingface:gigaword', 'huggingface:glucose', 'huggingface:glue', 'huggingface:gnad10', 'huggingface:go_emotions', 'huggingface:gooaq', 'huggingface:google_wellformed_query', 'huggingface:grail_qa', 'huggingface:great_code', 'huggingface:greek_legal_code', 'huggingface:gsm8k', 'huggingface:guardian_authorship', 'huggingface:gutenberg_time', 'huggingface:hans', 'huggingface:hansards', 'huggingface:hard', 'huggingface:harem', 'huggingface:has_part', 'huggingface:hate_offensive', 'huggingface:hate_speech18', 'huggingface:hate_speech_filipino', 'huggingface:hate_speech_offensive', 'huggingface:hate_speech_pl', 'huggingface:hate_speech_portuguese', 'huggingface:hatexplain', 'huggingface:hausa_voa_ner', 'huggingface:hausa_voa_topics', 'huggingface:hda_nli_hindi', 'huggingface:head_qa', 'huggingface:health_fact', 'huggingface:hebrew_projectbenyehuda', 'huggingface:hebrew_sentiment', 'huggingface:hebrew_this_world', 'huggingface:hellaswag', 'huggingface:hendrycks_test', 'huggingface:hind_encorp', 'huggingface:hindi_discourse', 'huggingface:hippocorpus', 'huggingface:hkcancor', 'huggingface:hlgd', 'huggingface:hope_edi', 'huggingface:hotpot_qa', 'huggingface:hover', 'huggingface:hrenwac_para', 'huggingface:hrwac', 'huggingface:humicroedit', 'huggingface:hybrid_qa', 'huggingface:hyperpartisan_news_detection', 'huggingface:iapp_wiki_qa_squad', 'huggingface:id_clickbait', 'huggingface:id_liputan6', 'huggingface:id_nergrit_corpus', 'huggingface:id_newspapers_2018', 'huggingface:id_panl_bppt', 'huggingface:id_puisi', 'huggingface:igbo_english_machine_translation', 'huggingface:igbo_monolingual', 'huggingface:igbo_ner', 'huggingface:ilist', 'huggingface:imagenet-1k', 'huggingface:imagenet_sketch', 'huggingface:imdb', 'huggingface:imdb_urdu_reviews', 'huggingface:imppres', 'huggingface:indic_glue', 'huggingface:indonli', 'huggingface:indonlu', 'huggingface:inquisitive_qg', 'huggingface:interpress_news_category_tr', 'huggingface:interpress_news_category_tr_lite', 'huggingface:irc_disentangle', 'huggingface:isixhosa_ner_corpus', 'huggingface:isizulu_ner_corpus', 'huggingface:iwslt2017', 'huggingface:jeopardy', 'huggingface:jfleg', 'huggingface:jigsaw_toxicity_pred', 'huggingface:jigsaw_unintended_bias', 'huggingface:jnlpba', 'huggingface:journalists_questions', 'huggingface:kan_hope', 'huggingface:kannada_news', 'huggingface:kd_conv', 'huggingface:kde4', 'huggingface:kelm', 'huggingface:kilt_tasks', 'huggingface:kilt_wikipedia', 'huggingface:kinnews_kirnews', 'huggingface:klue', 'huggingface:kor_3i4k', 'huggingface:kor_hate', 'huggingface:kor_ner', 'huggingface:kor_nli', 'huggingface:kor_nlu', 'huggingface:kor_qpair', 'huggingface:kor_sae', 'huggingface:kor_sarcasm', 'huggingface:labr', 'huggingface:lama', 'huggingface:lambada', 'huggingface:large_spanish_corpus', 'huggingface:laroseda', 'huggingface:lc_quad', 'huggingface:lccc', 'huggingface:lener_br', 'huggingface:lex_glue', 'huggingface:liar', 'huggingface:librispeech_asr', 'huggingface:librispeech_lm', 'huggingface:limit', 'huggingface:lince', 'huggingface:linnaeus', 'huggingface:liveqa', 'huggingface:lj_speech', 'huggingface:lm1b', 'huggingface:lst20', 'huggingface:m_lama', 'huggingface:mac_morpho', 'huggingface:makhzan', 'huggingface:masakhaner', 'huggingface:math_dataset', 'huggingface:math_qa', 'huggingface:matinf', 'huggingface:mbpp', 'huggingface:mc4', 'huggingface:mc_taco', 'huggingface:md_gender_bias', 'huggingface:mdd', 'huggingface:med_hop', 'huggingface:medal', 'huggingface:medical_dialog', 'huggingface:medical_questions_pairs', 'huggingface:medmcqa', 'huggingface:menyo20k_mt', 'huggingface:meta_woz', 'huggingface:metashift', 'huggingface:metooma', 'huggingface:metrec', 'huggingface:miam', 'huggingface:mkb', 'huggingface:mkqa', 'huggingface:mlqa', 'huggingface:mlsum', 'huggingface:mnist', 'huggingface:mocha', 'huggingface:monash_tsf', 'huggingface:moroco', 'huggingface:movie_rationales', 'huggingface:mrqa', 'huggingface:ms_marco', 'huggingface:ms_terms', 'huggingface:msr_genomics_kbcomp', 'huggingface:msr_sqa', 'huggingface:msr_text_compression', 'huggingface:msr_zhen_translation_parity', 'huggingface:msra_ner', 'huggingface:mt_eng_vietnamese', 'huggingface:muchocine', 'huggingface:multi_booked', 'huggingface:multi_eurlex', 'huggingface:multi_news', 'huggingface:multi_nli', 'huggingface:multi_nli_mismatch', 'huggingface:multi_para_crawl', 'huggingface:multi_re_qa', 'huggingface:multi_woz_v22', 'huggingface:multi_x_science_sum', 'huggingface:multidoc2dial', 'huggingface:multilingual_librispeech', 'huggingface:mutual_friends', 'huggingface:mwsc', 'huggingface:myanmar_news', 'huggingface:narrativeqa', 'huggingface:narrativeqa_manual', 'huggingface:natural_questions', 'huggingface:ncbi_disease', 'huggingface:nchlt', 'huggingface:ncslgr', 'huggingface:nell', 'huggingface:neural_code_search', 'huggingface:news_commentary', 'huggingface:newsgroup', 'huggingface:newsph', 'huggingface:newsph_nli', 'huggingface:newspop', 'huggingface:newsqa', 'huggingface:newsroom', 'huggingface:nkjp-ner', 'huggingface:nli_tr', 'huggingface:nlu_evaluation_data', 'huggingface:norec', 'huggingface:norne', 'huggingface:norwegian_ner', 'huggingface:nq_open', 'huggingface:nsmc', 'huggingface:numer_sense', 'huggingface:numeric_fused_head', 'huggingface:oclar', 'huggingface:offcombr', 'huggingface:offenseval2020_tr', 'huggingface:offenseval_dravidian', 'huggingface:ofis_publik', 'huggingface:ohsumed', 'huggingface:ollie', 'huggingface:omp', 'huggingface:onestop_english', 'huggingface:onestop_qa', 'huggingface:open_subtitles', 'huggingface:openai_humaneval', 'huggingface:openbookqa', 'huggingface:openslr', 'huggingface:openwebtext', 'huggingface:opinosis', 'huggingface:opus100', 'huggingface:opus_books', 'huggingface:opus_dgt', 'huggingface:opus_dogc', 'huggingface:opus_elhuyar', 'huggingface:opus_euconst', 'huggingface:opus_finlex', 'huggingface:opus_fiskmo', 'huggingface:opus_gnome', 'huggingface:opus_infopankki', 'huggingface:opus_memat', 'huggingface:opus_montenegrinsubs', 'huggingface:opus_openoffice', 'huggingface:opus_paracrawl', 'huggingface:opus_rf', 'huggingface:opus_tedtalks', 'huggingface:opus_ubuntu', 'huggingface:opus_wikipedia', 'huggingface:opus_xhosanavy', 'huggingface:orange_sum', 'huggingface:oscar', 'huggingface:para_crawl', 'huggingface:para_pat', 'huggingface:parsinlu_reading_comprehension', 'huggingface:pass', 'huggingface:paws', 'huggingface:paws-x', 'huggingface:pec', 'huggingface:peer_read', 'huggingface:peoples_daily_ner', 'huggingface:per_sent', 'huggingface:persian_ner', 'huggingface:pg19', 'huggingface:php', 'huggingface:piaf', 'huggingface:pib', 'huggingface:piqa', 'huggingface:pn_summary', 'huggingface:poem_sentiment', 'huggingface:polemo2', 'huggingface:poleval2019_cyberbullying', 'huggingface:poleval2019_mt', 'huggingface:polsum', 'huggingface:polyglot_ner', 'huggingface:prachathai67k', 'huggingface:pragmeval', 'huggingface:proto_qa', 'huggingface:psc', 'huggingface:ptb_text_only', 'huggingface:pubmed', 'huggingface:pubmed_qa', 'huggingface:py_ast', 'huggingface:qa4mre', 'huggingface:qa_srl', 'huggingface:qa_zre', 'huggingface:qangaroo', 'huggingface:qanta', 'huggingface:qasc', 'huggingface:qasper', 'huggingface:qed', 'huggingface:qed_amara', 'huggingface:quac', 'huggingface:quail', 'huggingface:quarel', 'huggingface:quartz', 'huggingface:quickdraw', 'huggingface:quora', 'huggingface:quoref', 'huggingface:race', 'huggingface:re_dial', 'huggingface:reasoning_bg', 'huggingface:recipe_nlg', 'huggingface:reclor', 'huggingface:red_caps', 'huggingface:reddit', 'huggingface:reddit_tifu', 'huggingface:refresd', 'huggingface:reuters21578', 'huggingface:riddle_sense', 'huggingface:ro_sent', 'huggingface:ro_sts', 'huggingface:ro_sts_parallel', 'huggingface:roman_urdu', 'huggingface:roman_urdu_hate_speech', 'huggingface:ronec', 'huggingface:ropes', 'huggingface:rotten_tomatoes', 'huggingface:russian_super_glue', 'huggingface:rvl_cdip', 'huggingface:s2orc', 'huggingface:samsum', 'huggingface:sanskrit_classic', 'huggingface:saudinewsnet', 'huggingface:sberquad', 'huggingface:sbu_captions', 'huggingface:scan', 'huggingface:scb_mt_enth_2020', 'huggingface:scene_parse_150', 'huggingface:schema_guided_dstc8', 'huggingface:scicite', 'huggingface:scielo', 'huggingface:scientific_papers', 'huggingface:scifact', 'huggingface:sciq', 'huggingface:scitail', 'huggingface:scitldr', 'huggingface:search_qa', 'huggingface:sede', 'huggingface:selqa', 'huggingface:sem_eval_2010_task_8', 'huggingface:sem_eval_2014_task_1', 'huggingface:sem_eval_2018_task_1', 'huggingface:sem_eval_2020_task_11', 'huggingface:sent_comp', 'huggingface:senti_lex', 'huggingface:senti_ws', 'huggingface:sentiment140', 'huggingface:sepedi_ner', 'huggingface:sesotho_ner_corpus', 'huggingface:setimes', 'huggingface:setswana_ner_corpus', 'huggingface:sharc', 'huggingface:sharc_modified', 'huggingface:sick', 'huggingface:silicone', 'huggingface:simple_questions_v2', 'huggingface:siswati_ner_corpus', 'huggingface:smartdata', 'huggingface:sms_spam', 'huggingface:snips_built_in_intents', 'huggingface:snli', 'huggingface:snow_simplified_japanese_corpus', 'huggingface:so_stacksample', 'huggingface:social_bias_frames', 'huggingface:social_i_qa', 'huggingface:sofc_materials_articles', 'huggingface:sogou_news', 'huggingface:spanish_billion_words', 'huggingface:spc', 'huggingface:species_800', 'huggingface:speech_commands', 'huggingface:spider', 'huggingface:squad', 'huggingface:squad_adversarial', 'huggingface:squad_es', 'huggingface:squad_it', 'huggingface:squad_kor_v1', 'huggingface:squad_kor_v2', 'huggingface:squad_v1_pt', 'huggingface:squad_v2', 'huggingface:squadshifts', 'huggingface:srwac', 'huggingface:sst', 'huggingface:stereoset', 'huggingface:story_cloze', 'huggingface:stsb_mt_sv', 'huggingface:stsb_multi_mt', 'huggingface:style_change_detection', 'huggingface:subjqa', 'huggingface:super_glue', 'huggingface:superb', 'huggingface:svhn', 'huggingface:swag', 'huggingface:swahili', 'huggingface:swahili_news', 'huggingface:swda', 'huggingface:swedish_medical_ner', 'huggingface:swedish_ner_corpus', 'huggingface:swedish_reviews', 'huggingface:swiss_judgment_prediction', 'huggingface:tab_fact', 'huggingface:tamilmixsentiment', 'huggingface:tanzil', 'huggingface:tapaco', 'huggingface:tashkeela', 'huggingface:taskmaster1', 'huggingface:taskmaster2', 'huggingface:taskmaster3', 'huggingface:tatoeba', 'huggingface:ted_hrlr', 'huggingface:ted_iwlst2013', 'huggingface:ted_multi', 'huggingface:ted_talks_iwslt', 'huggingface:telugu_books', 'huggingface:telugu_news', 'huggingface:tep_en_fa_para', 'huggingface:text2log', 'huggingface:textvqa', 'huggingface:thai_toxicity_tweet', 'huggingface:thainer', 'huggingface:thaiqa_squad', 'huggingface:thaisum', 'huggingface:the_pile', 'huggingface:the_pile_books3', 'huggingface:the_pile_openwebtext2', 'huggingface:the_pile_stack_exchange', 'huggingface:tilde_model', 'huggingface:time_dial', 'huggingface:times_of_india_news_headlines', 'huggingface:timit_asr', 'huggingface:tiny_shakespeare', 'huggingface:tlc', 'huggingface:tmu_gfm_dataset', 'huggingface:tne', 'huggingface:told-br', 'huggingface:totto', 'huggingface:trec', 'huggingface:trivia_qa', 'huggingface:truthful_qa', 'huggingface:tsac', 'huggingface:ttc4900', 'huggingface:tunizi', 'huggingface:tuple_ie', 'huggingface:turk', 'huggingface:turkic_xwmt', 'huggingface:turkish_movie_sentiment', 'huggingface:turkish_ner', 'huggingface:turkish_product_reviews', 'huggingface:turkish_shrinked_ner', 'huggingface:turku_ner_corpus', 'huggingface:tweet_eval', 'huggingface:tweet_qa', 'huggingface:tweets_ar_en_parallel', 'huggingface:tweets_hate_speech_detection', 'huggingface:twi_text_c3', 'huggingface:twi_wordsim353', 'huggingface:tydiqa', 'huggingface:ubuntu_dialogs_corpus', 'huggingface:udhr', 'huggingface:um005', 'huggingface:un_ga', 'huggingface:un_multi', 'huggingface:un_pc', 'huggingface:universal_dependencies', 'huggingface:universal_morphologies', 'huggingface:urdu_fake_news', 'huggingface:urdu_sentiment_corpus', 'huggingface:vctk', 'huggingface:visual_genome', 'huggingface:vivos', 'huggingface:web_nlg', 'huggingface:web_of_science', 'huggingface:web_questions', 'huggingface:weibo_ner', 'huggingface:wi_locness', 'huggingface:wider_face', 'huggingface:wiki40b', 'huggingface:wiki_asp', 'huggingface:wiki_atomic_edits', 'huggingface:wiki_auto', 'huggingface:wiki_bio', 'huggingface:wiki_dpr', 'huggingface:wiki_hop', 'huggingface:wiki_lingua', 'huggingface:wiki_movies', 'huggingface:wiki_qa', 'huggingface:wiki_qa_ar', 'huggingface:wiki_snippets', 'huggingface:wiki_source', 'huggingface:wiki_split', 'huggingface:wiki_summary', 'huggingface:wikiann', 'huggingface:wikicorpus', 'huggingface:wikihow', 'huggingface:wikipedia', 'huggingface:wikisql', 'huggingface:wikitablequestions', 'huggingface:wikitext', 'huggingface:wikitext_tl39', 'huggingface:wili_2018', 'huggingface:wino_bias', 'huggingface:winograd_wsc', 'huggingface:winogrande', 'huggingface:wiqa', 'huggingface:wisesight1000', 'huggingface:wisesight_sentiment', 'huggingface:wmt14', 'huggingface:wmt15', 'huggingface:wmt16', 'huggingface:wmt17', 'huggingface:wmt18', 'huggingface:wmt19', 'huggingface:wmt20_mlqe_task1', 'huggingface:wmt20_mlqe_task2', 'huggingface:wmt20_mlqe_task3', 'huggingface:wmt_t2t', 'huggingface:wnut_17', 'huggingface:wongnai_reviews', 'huggingface:woz_dialogue', 'huggingface:wrbsc', 'huggingface:x_stance', 'huggingface:xcopa', 'huggingface:xcsr', 'huggingface:xed_en_fi', 'huggingface:xglue', 'huggingface:xnli', 'huggingface:xor_tydi_qa', 'huggingface:xquad', 'huggingface:xquad_r', 'huggingface:xsum', 'huggingface:xsum_factuality', 'huggingface:xtreme', 'huggingface:yahoo_answers_qa', 'huggingface:yahoo_answers_topics', 'huggingface:yelp_polarity', 'huggingface:yelp_review_full', 'huggingface:yoruba_bbc_topics', 'huggingface:yoruba_gv_ner', 'huggingface:yoruba_text_c3', 'huggingface:yoruba_wordsim353', 'huggingface:youtube_caption_corrections', 'huggingface:zest', 'kubric:kubric_frames', 'kubric:movi_a', 'kubric:movi_b', 'kubric:movi_c', 'kubric:movi_d', 'kubric:movi_e', 'kubric:movi_f', 'kubric:msn_easy', 'kubric:multi_shapenet_frames', 'kubric:nerf_synthetic_frames', 'kubric:nerf_synthetic_scenes', 'kubric:shapenet_pretraining', 'robotics:agent_aware_affordances', 'robotics:aloha_mobile', 'robotics:asu_table_top_converted_externally_to_rlds', 'robotics:austin_buds_dataset_converted_externally_to_rlds', 'robotics:austin_sailor_dataset_converted_externally_to_rlds', 'robotics:austin_sirius_dataset_converted_externally_to_rlds', 'robotics:bc_z', 'robotics:berkeley_autolab_ur5', 'robotics:berkeley_cable_routing', 'robotics:berkeley_fanuc_manipulation', 'robotics:berkeley_gnm_cory_hall', 'robotics:berkeley_gnm_recon', 'robotics:berkeley_gnm_sac_son', 'robotics:berkeley_mvp_converted_externally_to_rlds', 'robotics:berkeley_rpt_converted_externally_to_rlds', 'robotics:bridge', 'robotics:bridge_data_msr', 'robotics:bridge_data_v2', 'robotics:cmu_franka_exploration_dataset_converted_externally_to_rlds', 'robotics:cmu_play_fusion', 'robotics:cmu_playing_with_food', 'robotics:cmu_stretch', 'robotics:columbia_cairlab_pusht_real', 'robotics:conq_hose_manipulation', 'robotics:dlr_edan_shared_control_converted_externally_to_rlds', 'robotics:dlr_sara_grid_clamp_converted_externally_to_rlds', 'robotics:dlr_sara_pour_converted_externally_to_rlds', 'robotics:dobbe', 'robotics:droid', 'robotics:droid_100', 'robotics:droid_raw', 'robotics:eth_agent_affordances', 'robotics:fanuc_manipulation_v2', 'robotics:fmb', 'robotics:fractal20220817_data', 'robotics:furniture_bench_dataset_converted_externally_to_rlds', 'robotics:iamlab_cmu_pickup_insert_converted_externally_to_rlds', 'robotics:imperial_wrist_dataset', 'robotics:imperialcollege_sawyer_wrist_cam', 'robotics:io_ai_tech', 'robotics:jaco_play', 'robotics:kaist_nonprehensile_converted_externally_to_rlds', 'robotics:kuka', 'robotics:language_table', 'robotics:language_table_blocktoabsolute_oracle_sim', 'robotics:language_table_blocktoblock_4block_sim', 'robotics:language_table_blocktoblock_oracle_sim', 'robotics:language_table_blocktoblock_sim', 'robotics:language_table_blocktoblockrelative_oracle_sim', 'robotics:language_table_blocktorelative_oracle_sim', 'robotics:language_table_checkpoints', 'robotics:language_table_separate_oracle_sim', 'robotics:language_table_sim', 'robotics:maniskill_dataset_converted_externally_to_rlds', 'robotics:mimic_play', 'robotics:mt_opt_rlds', 'robotics:mt_opt_sd', 'robotics:mutex_dataset', 'robotics:nyu_door_opening_surprising_effectiveness', 'robotics:nyu_franka_play_dataset_converted_externally_to_rlds', 'robotics:nyu_rot_dataset_converted_externally_to_rlds', 'robotics:open_x_embodiment_and_rt_x_oss', 'robotics:plex_robosuite', 'robotics:qut_dexterous_manpulation', 'robotics:robo_net', 'robotics:robo_set', 'robotics:robot_vqa', 'robotics:roboturk', 'robotics:spoc', 'robotics:stanford_hydra_dataset_converted_externally_to_rlds', 'robotics:stanford_kuka_multimodal_dataset_converted_externally_to_rlds', 'robotics:stanford_mask_vit_converted_externally_to_rlds', 'robotics:stanford_robocook_converted_externally_to_rlds', 'robotics:taco_play', 'robotics:tidybot', 'robotics:tokyo_u_lsmo_converted_externally_to_rlds', 'robotics:toto', 'robotics:ucsd_kitchen_dataset_converted_externally_to_rlds', 'robotics:ucsd_pick_and_place_dataset_converted_externally_to_rlds', 'robotics:uiuc_d3field', 'robotics:usc_cloth_sim_converted_externally_to_rlds', 'robotics:utaustin_mutex', 'robotics:utokyo_pr2_opening_fridge_converted_externally_to_rlds', 'robotics:utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds', 'robotics:utokyo_saytap_converted_externally_to_rlds', 'robotics:utokyo_xarm_bimanual_converted_externally_to_rlds', 'robotics:utokyo_xarm_pick_and_place_converted_externally_to_rlds', 'robotics:vima_converted_externally_to_rlds', 'robotics:viola']\n"
     ]
    }
   ],
   "source": [
    "print(tfds.list_builders())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:08:31.404165: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "mnist_train = mnist_train.repeat(5).batch(32).prefetch(1)\n",
    "for item in mnist_train:\n",
    "    images = item[\"image\"]\n",
    "    labels = item[\"label\"]\n",
    "    for index in range(5):\n",
    "        plt.subplot(1, 5, index + 1)\n",
    "        image = images[index, ..., 0]\n",
    "        label = labels[index].numpy()\n",
    "        plt.imshow(image, cmap=\"binary\")\n",
    "        plt.title(label)\n",
    "        plt.axis(\"off\")\n",
    "    break # just showing part of the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 28, 28, 1)\n",
      "[4 1 0 7 8 1 2 7 1 6 6 4 7 7 3 3 7 9 9 1 0 6 6 9 9 4 8 9 4 7 3 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:08:31.714064: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-11-12 16:08:31.717961: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-11-12 16:08:31.718669: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "datasets = tfds.load(name=\"mnist\")\n",
    "mnist_train, mnist_test = datasets[\"train\"], datasets[\"test\"]\n",
    "mnist_train = mnist_train.repeat(5).batch(32)\n",
    "mnist_train = mnist_train.map(lambda items: (items[\"image\"], items[\"label\"]))\n",
    "mnist_train = mnist_train.prefetch(1)\n",
    "for images, labels in mnist_train.take(1):\n",
    "    print(images.shape)\n",
    "    print(labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.8045 - loss: 40.9162\n",
      "Epoch 2/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 977us/step - accuracy: 0.8686 - loss: 24.7762\n",
      "Epoch 3/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8722 - loss: 24.2740\n",
      "Epoch 4/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8756 - loss: 23.6455\n",
      "Epoch 5/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8771 - loss: 23.4065\n",
      "Epoch 6/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8789 - loss: 22.9026\n",
      "Epoch 7/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8821 - loss: 22.5999\n",
      "Epoch 8/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8830 - loss: 22.4855\n",
      "Epoch 9/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.8819 - loss: 23.0062\n",
      "Epoch 10/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8825 - loss: 22.1775\n",
      "Epoch 11/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8841 - loss: 21.6831\n",
      "Epoch 12/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8856 - loss: 21.7231\n",
      "Epoch 13/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8854 - loss: 21.8395\n",
      "Epoch 14/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8848 - loss: 21.9435\n",
      "Epoch 15/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8845 - loss: 22.1649\n",
      "Epoch 16/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8853 - loss: 21.7078\n",
      "Epoch 17/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 951us/step - accuracy: 0.8863 - loss: 21.4721\n",
      "Epoch 18/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8867 - loss: 21.8504\n",
      "Epoch 19/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8853 - loss: 21.9779\n",
      "Epoch 20/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.8880 - loss: 21.4999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1896c0f90>"
      ]
     },
     "execution_count": 859,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = tfds.load(name=\"mnist\", batch_size=32, as_supervised=True)\n",
    "mnist_train = datasets[\"train\"].repeat().prefetch(1)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28, 1]),\n",
    "    keras.layers.Lambda(lambda images: tf.cast(images, tf.float32)),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(mnist_train, steps_per_epoch=60000 // 32, epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
