{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "import numpy as np\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf \n",
    "# from tensorflow import keras\n",
    "# import numpy as np\n",
    "# import ssl\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation Functions and their derivatives\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def tanh(z):\n",
    "    return 2 * sigmoid(2 * z) - 1\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)\n",
    "\n",
    "def elu(z, alpha=00.1):\n",
    "    if z < 0:\n",
    "        return alpha*np.exp(z)-1\n",
    "    elif z >= 1:\n",
    "        return z\n",
    "\n",
    "def softplus(z):\n",
    "    return np.log(1 + np.exp(z))\n",
    "\n",
    "def derivative(f, z, eps = 0.00001):\n",
    "    return (f(z + eps) - f(z - eps)) / (2*eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Constant',\n",
       " 'GlorotNormal',\n",
       " 'GlorotUniform',\n",
       " 'HeNormal',\n",
       " 'HeUniform',\n",
       " 'Identity',\n",
       " 'IdentityInitializer',\n",
       " 'Initializer',\n",
       " 'LecunNormal',\n",
       " 'LecunUniform',\n",
       " 'Ones',\n",
       " 'Orthogonal',\n",
       " 'OrthogonalInitializer',\n",
       " 'RandomNormal',\n",
       " 'RandomUniform',\n",
       " 'TruncatedNormal',\n",
       " 'VarianceScaling',\n",
       " 'Zeros',\n",
       " 'constant',\n",
       " 'deserialize',\n",
       " 'get',\n",
       " 'glorot_normal',\n",
       " 'glorot_uniform',\n",
       " 'he_normal',\n",
       " 'he_uniform',\n",
       " 'identity',\n",
       " 'lecun_normal',\n",
       " 'lecun_uniform',\n",
       " 'ones',\n",
       " 'orthogonal',\n",
       " 'random_normal',\n",
       " 'random_uniform',\n",
       " 'serialize',\n",
       " 'truncated_normal',\n",
       " 'variance_scaling',\n",
       " 'zeros']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_initializers = [name for name in dir(keras.initializers) if not name.startswith(\"_\")]\n",
    "keras_initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deserialize',\n",
       " 'elu',\n",
       " 'exponential',\n",
       " 'gelu',\n",
       " 'get',\n",
       " 'hard_sigmoid',\n",
       " 'hard_silu',\n",
       " 'hard_swish',\n",
       " 'leaky_relu',\n",
       " 'linear',\n",
       " 'log_softmax',\n",
       " 'mish',\n",
       " 'relu',\n",
       " 'relu6',\n",
       " 'selu',\n",
       " 'serialize',\n",
       " 'sigmoid',\n",
       " 'silu',\n",
       " 'softmax',\n",
       " 'softplus',\n",
       " 'softsign',\n",
       " 'swish',\n",
       " 'tanh']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_activations = [name for name in dir(keras.activations ) if not name.startswith(\"_\")]\n",
    "keras_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LeakyReLU', 'PReLU', 'ReLU', 'ThresholdedReLU']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu_varaints = [m for m in dir(keras.layers) if \"relu\" in m.lower()]\n",
    "relu_varaints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.4920 - loss: 1.6239 - val_accuracy: 0.7240 - val_loss: 0.8586\n",
      "Epoch 2/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7260 - loss: 0.8255 - val_accuracy: 0.7678 - val_loss: 0.7043\n",
      "Epoch 3/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7649 - loss: 0.6991 - val_accuracy: 0.7906 - val_loss: 0.6335\n",
      "Epoch 4/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7876 - loss: 0.6350 - val_accuracy: 0.8070 - val_loss: 0.5896\n",
      "Epoch 5/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8021 - loss: 0.5935 - val_accuracy: 0.8162 - val_loss: 0.5590\n",
      "Epoch 6/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8110 - loss: 0.5640 - val_accuracy: 0.8234 - val_loss: 0.5363\n",
      "Epoch 7/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8178 - loss: 0.5417 - val_accuracy: 0.8300 - val_loss: 0.5186\n",
      "Epoch 8/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8223 - loss: 0.5240 - val_accuracy: 0.8336 - val_loss: 0.5043\n",
      "Epoch 9/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8278 - loss: 0.5097 - val_accuracy: 0.8386 - val_loss: 0.4925\n",
      "Epoch 10/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8311 - loss: 0.4978 - val_accuracy: 0.8416 - val_loss: 0.4826\n"
     ]
    }
   ],
   "source": [
    "#Training Fashion mnist with Leaky Relu\n",
    "fashion_mnist =  keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "X_valid, X_train  = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train  = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "leaky_relu_model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(300, kernel_initializer='he_normal'),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer='he_normal'),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001)\n",
    "leaky_relu_model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "leaky_relu_history = leaky_relu_model.fit(X_train, y_train, epochs=10,\n",
    "          validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(leaky_relu_history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.4735 - loss: 1.6915 - val_accuracy: 0.7142 - val_loss: 0.9023\n",
      "Epoch 2/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7184 - loss: 0.8609 - val_accuracy: 0.7632 - val_loss: 0.7283\n",
      "Epoch 3/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7596 - loss: 0.7212 - val_accuracy: 0.7910 - val_loss: 0.6504\n",
      "Epoch 4/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7846 - loss: 0.6507 - val_accuracy: 0.8064 - val_loss: 0.6015\n",
      "Epoch 5/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8014 - loss: 0.6047 - val_accuracy: 0.8188 - val_loss: 0.5672\n",
      "Epoch 6/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8112 - loss: 0.5720 - val_accuracy: 0.8256 - val_loss: 0.5419\n",
      "Epoch 7/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8180 - loss: 0.5472 - val_accuracy: 0.8326 - val_loss: 0.5221\n",
      "Epoch 8/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8247 - loss: 0.5277 - val_accuracy: 0.8370 - val_loss: 0.5061\n",
      "Epoch 9/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8295 - loss: 0.5118 - val_accuracy: 0.8428 - val_loss: 0.4930\n",
      "Epoch 10/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8333 - loss: 0.4985 - val_accuracy: 0.8446 - val_loss: 0.4820\n"
     ]
    }
   ],
   "source": [
    "#Training Fashion mnist with PReLU\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "prelu_model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(300, kernel_initializer='he_normal'),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer='he_normal'),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001)\n",
    "prelu_model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "prelu_history = prelu_model.fit(X_train, y_train, epochs=10,\n",
    "          validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.5210 - loss: 1.4740 - val_accuracy: 0.7484 - val_loss: 0.7788\n",
      "Epoch 2/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7504 - loss: 0.7489 - val_accuracy: 0.7802 - val_loss: 0.6517\n",
      "Epoch 3/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7841 - loss: 0.6436 - val_accuracy: 0.8006 - val_loss: 0.5930\n",
      "Epoch 4/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8004 - loss: 0.5904 - val_accuracy: 0.8142 - val_loss: 0.5573\n",
      "Epoch 5/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8103 - loss: 0.5569 - val_accuracy: 0.8208 - val_loss: 0.5327\n",
      "Epoch 6/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8179 - loss: 0.5333 - val_accuracy: 0.8250 - val_loss: 0.5146\n",
      "Epoch 7/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8226 - loss: 0.5156 - val_accuracy: 0.8306 - val_loss: 0.5005\n",
      "Epoch 8/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8277 - loss: 0.5017 - val_accuracy: 0.8340 - val_loss: 0.4892\n",
      "Epoch 9/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8313 - loss: 0.4903 - val_accuracy: 0.8370 - val_loss: 0.4798\n",
      "Epoch 10/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8348 - loss: 0.4807 - val_accuracy: 0.8404 - val_loss: 0.4719\n"
     ]
    }
   ],
   "source": [
    "#Training Fashion mnist with ELU\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "elu_model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001)\n",
    "elu_model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "elu_history = elu_model.fit(X_train, y_train, epochs=10,\n",
    "          validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: mean -0.00, std deviation 1.00\n",
      "Layer 100: mean 0.02, std deviation 0.96\n",
      "Layer 200: mean 0.01, std deviation 0.90\n",
      "Layer 300: mean -0.02, std deviation 0.92\n",
      "Layer 400: mean 0.05, std deviation 0.89\n",
      "Layer 500: mean 0.01, std deviation 0.93\n",
      "Layer 600: mean 0.02, std deviation 0.92\n",
      "Layer 700: mean -0.02, std deviation 0.90\n",
      "Layer 800: mean 0.05, std deviation 0.83\n",
      "Layer 900: mean 0.02, std deviation 1.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy.special import erfc\n",
    "# alpha and scale to self normalize with mean 0 and standard deviation 1\n",
    "alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1/np.sqrt(2)) * np.exp(1/2) - 1)\n",
    "scale_0_1 = (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2 * np.pi) * (2 * erfc(np.sqrt(2))*np.e**2 + np.pi*erfc(1/np.sqrt(2))**2*np.e - 2*(2+np.pi)*erfc(1/np.sqrt(2))*np.sqrt(np.e)+np.pi+2)**(-1/2)\n",
    "# Element-wise ELU function to handle array input\n",
    "\n",
    "def elu(z, alpha):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)\n",
    "\n",
    "def selu(z, scale=scale_0_1, alpha=alpha_0_1):\n",
    "    return scale * elu(z, alpha)\n",
    "\n",
    "np.random.seed(42)\n",
    "Z = np.random.normal(size=(500, 100)) # standardized inputs\n",
    "for layer in range(1000):\n",
    "    W = np.random.normal(size=(100, 100), scale=np.sqrt(1 / 100)) # LeCun initialization\n",
    "    Z = selu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis=0).mean()\n",
    "    stds = np.std(Z, axis=0).mean()\n",
    "    if layer % 100 == 0:\n",
    "        print(\"Layer {}: mean {:.2f}, std deviation {:.2f}\".format(layer, means, stds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 15ms/step - accuracy: 0.2656 - loss: 1.9069 - val_accuracy: 0.6378 - val_loss: 0.9075\n",
      "Epoch 2/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 15ms/step - accuracy: 0.6887 - loss: 0.7940 - val_accuracy: 0.7298 - val_loss: 0.6810\n",
      "Epoch 3/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 15ms/step - accuracy: 0.7615 - loss: 0.6349 - val_accuracy: 0.7488 - val_loss: 0.6640\n",
      "Epoch 4/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 10ms/step - accuracy: 0.7844 - loss: 0.5664 - val_accuracy: 0.7578 - val_loss: 0.6335\n",
      "Epoch 5/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 13ms/step - accuracy: 0.8042 - loss: 0.5231 - val_accuracy: 0.8078 - val_loss: 0.5204\n",
      "Epoch 6/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 10ms/step - accuracy: 0.8235 - loss: 0.4913 - val_accuracy: 0.8058 - val_loss: 0.5286\n",
      "Epoch 7/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 13ms/step - accuracy: 0.8313 - loss: 0.4807 - val_accuracy: 0.8078 - val_loss: 0.5187\n",
      "Epoch 8/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - accuracy: 0.8289 - loss: 0.4805 - val_accuracy: 0.8360 - val_loss: 0.4619\n",
      "Epoch 9/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.8481 - loss: 0.4316 - val_accuracy: 0.8020 - val_loss: 0.5305\n",
      "Epoch 10/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.8506 - loss: 0.4198 - val_accuracy: 0.8124 - val_loss: 0.4952\n"
     ]
    }
   ],
   "source": [
    "#Training Fashion mnist with SELU with 100 Hidden Layers\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "selu_model = keras.models.Sequential()\n",
    "selu_model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "selu_model.add(keras.layers.Dense(300, activation=\"selu\",\n",
    "                             kernel_initializer=\"lecun_normal\"))\n",
    "for layer in range(99):\n",
    "    selu_model.add(keras.layers.Dense(100, activation=\"selu\",\n",
    "                                 kernel_initializer=\"lecun_normal\"))\n",
    "selu_model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "selu_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds\n",
    "\n",
    "selu_history = selu_model.fit(X_train, y_train, epochs=10,\n",
    "          validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_71\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_71\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_68 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_40          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1396 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_41          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,200</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1397 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_42          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1398 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_68 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_40          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │         \u001b[38;5;34m3,136\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1396 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_41          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │         \u001b[38;5;34m1,200\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1397 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_42          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │           \u001b[38;5;34m400\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1398 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">271,346</span> (1.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m271,346\u001b[0m (1.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">268,978</span> (1.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m268,978\u001b[0m (1.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,368</span> (9.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,368\u001b[0m (9.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.6172 - loss: 1.1550 - val_accuracy: 0.8226 - val_loss: 0.5449\n",
      "Epoch 2/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8039 - loss: 0.5819 - val_accuracy: 0.8422 - val_loss: 0.4742\n",
      "Epoch 3/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8259 - loss: 0.5116 - val_accuracy: 0.8482 - val_loss: 0.4413\n",
      "Epoch 4/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8382 - loss: 0.4733 - val_accuracy: 0.8524 - val_loss: 0.4209\n",
      "Epoch 5/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8473 - loss: 0.4470 - val_accuracy: 0.8558 - val_loss: 0.4067\n",
      "Epoch 6/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8537 - loss: 0.4270 - val_accuracy: 0.8584 - val_loss: 0.3961\n",
      "Epoch 7/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8586 - loss: 0.4107 - val_accuracy: 0.8624 - val_loss: 0.3876\n",
      "Epoch 8/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8629 - loss: 0.3967 - val_accuracy: 0.8640 - val_loss: 0.3805\n",
      "Epoch 9/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8666 - loss: 0.3845 - val_accuracy: 0.8650 - val_loss: 0.3745\n",
      "Epoch 10/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8709 - loss: 0.3736 - val_accuracy: 0.8668 - val_loss: 0.3695\n"
     ]
    }
   ],
   "source": [
    "#Batch Normalization\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "bn_model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "bn_model.summary()\n",
    "\n",
    "bn_model.compile(loss='sparse_categorical_crossentropy',\n",
    "                 optimizer=keras.optimizers.SGD(learning_rate=0.001),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "bn_history = bn_model.fit(X_train, y_train, epochs=10,\n",
    "                          validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_72\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_72\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_69 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_43          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1399 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_44          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,200</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_45          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,200</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1400 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_46          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1401 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_69 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_43          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │         \u001b[38;5;34m3,136\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1399 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_44          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │         \u001b[38;5;34m1,200\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_14 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_45          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │         \u001b[38;5;34m1,200\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1400 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_46          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │           \u001b[38;5;34m400\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_15 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1401 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">272,146</span> (1.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m272,146\u001b[0m (1.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">269,178</span> (1.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m269,178\u001b[0m (1.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,968</span> (11.59 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,968\u001b[0m (11.59 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.5514 - loss: 1.3989 - val_accuracy: 0.7932 - val_loss: 0.6736\n",
      "Epoch 2/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7759 - loss: 0.7153 - val_accuracy: 0.8244 - val_loss: 0.5558\n",
      "Epoch 3/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8029 - loss: 0.6100 - val_accuracy: 0.8334 - val_loss: 0.5003\n",
      "Epoch 4/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8171 - loss: 0.5555 - val_accuracy: 0.8434 - val_loss: 0.4669\n",
      "Epoch 5/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8270 - loss: 0.5202 - val_accuracy: 0.8494 - val_loss: 0.4442\n",
      "Epoch 6/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - accuracy: 0.8348 - loss: 0.4946 - val_accuracy: 0.8544 - val_loss: 0.4275\n",
      "Epoch 7/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.8402 - loss: 0.4746 - val_accuracy: 0.8576 - val_loss: 0.4146\n",
      "Epoch 8/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8452 - loss: 0.4583 - val_accuracy: 0.8594 - val_loss: 0.4041\n",
      "Epoch 9/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.8485 - loss: 0.4445 - val_accuracy: 0.8604 - val_loss: 0.3953\n",
      "Epoch 10/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8527 - loss: 0.4324 - val_accuracy: 0.8634 - val_loss: 0.3877\n"
     ]
    }
   ],
   "source": [
    "#Batch Normalization - BEFORE activation function\n",
    "bn_before_model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "bn_before_model.summary()\n",
    "\n",
    "bn_before_model.compile(loss='sparse_categorical_crossentropy',\n",
    "                 optimizer=keras.optimizers.SGD(learning_rate=0.001),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "bn_before_history = bn_before_model.fit(X_train, y_train, epochs=10,\n",
    "                          validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.6905 - loss: 0.9651 - val_accuracy: 0.8300 - val_loss: 0.5069\n",
      "Epoch 2/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8248 - loss: 0.4960 - val_accuracy: 0.8508 - val_loss: 0.4428\n",
      "Epoch 3/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8433 - loss: 0.4446 - val_accuracy: 0.8580 - val_loss: 0.4130\n",
      "Epoch 4/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8535 - loss: 0.4156 - val_accuracy: 0.8646 - val_loss: 0.3934\n",
      "Epoch 5/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8608 - loss: 0.3943 - val_accuracy: 0.8686 - val_loss: 0.3781\n",
      "Epoch 6/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8659 - loss: 0.3774 - val_accuracy: 0.8696 - val_loss: 0.3670\n",
      "Epoch 7/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8708 - loss: 0.3633 - val_accuracy: 0.8740 - val_loss: 0.3566\n",
      "Epoch 8/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8753 - loss: 0.3511 - val_accuracy: 0.8770 - val_loss: 0.3485\n",
      "Epoch 9/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8783 - loss: 0.3405 - val_accuracy: 0.8790 - val_loss: 0.3415\n",
      "Epoch 10/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8817 - loss: 0.3309 - val_accuracy: 0.8822 - val_loss: 0.3356\n"
     ]
    }
   ],
   "source": [
    "#RELU with Gradient Clipping\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "gc_model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(300, activation='relu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "\n",
    "gc_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "gc_history = gc_model.fit(X_train, y_train, epochs=10,\n",
    "          validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reusing Pretrained Layers\n",
    "\n",
    "#Split Fashion Mnist into \n",
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1375/1375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7282 - loss: 0.8346 - val_accuracy: 0.8749 - val_loss: 0.3815\n",
      "Epoch 2/20\n",
      "\u001b[1m1375/1375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8736 - loss: 0.3670 - val_accuracy: 0.8901 - val_loss: 0.3288\n",
      "Epoch 3/20\n",
      "\u001b[1m1375/1375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8855 - loss: 0.3239 - val_accuracy: 0.8966 - val_loss: 0.3070\n",
      "Epoch 4/20\n",
      "\u001b[1m1375/1375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8935 - loss: 0.3024 - val_accuracy: 0.8981 - val_loss: 0.2946\n",
      "Epoch 5/20\n",
      "\u001b[1m1375/1375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8987 - loss: 0.2881 - val_accuracy: 0.9036 - val_loss: 0.2854\n",
      "Epoch 6/20\n",
      "\u001b[1m1375/1375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9038 - loss: 0.2777 - val_accuracy: 0.9036 - val_loss: 0.2795\n",
      "Epoch 7/20\n",
      "\u001b[1m1375/1375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9070 - loss: 0.2696 - val_accuracy: 0.9048 - val_loss: 0.2747\n",
      "Epoch 8/20\n",
      "\u001b[1m1375/1375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9096 - loss: 0.2629 - val_accuracy: 0.9058 - val_loss: 0.2703\n",
      "Epoch 9/20\n",
      "\u001b[1m1375/1375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9119 - loss: 0.2572 - val_accuracy: 0.9068 - val_loss: 0.2668\n",
      "Epoch 10/20\n",
      "\u001b[1m1375/1375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9132 - loss: 0.2522 - val_accuracy: 0.9076 - val_loss: 0.2644\n",
      "Epoch 11/20\n",
      "\u001b[1m1375/1375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9142 - loss: 0.2479 - val_accuracy: 0.9081 - val_loss: 0.2613\n",
      "Epoch 12/20\n",
      "\u001b[1m1375/1375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9159 - loss: 0.2440 - val_accuracy: 0.9081 - val_loss: 0.2586\n",
      "Epoch 13/20\n",
      "\u001b[1m1375/1375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9172 - loss: 0.2404 - val_accuracy: 0.9078 - val_loss: 0.2560\n",
      "Epoch 14/20\n",
      "\u001b[1m1375/1375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9188 - loss: 0.2371 - val_accuracy: 0.9081 - val_loss: 0.2544\n",
      "Epoch 15/20\n",
      "\u001b[1m1375/1375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9200 - loss: 0.2341 - val_accuracy: 0.9088 - val_loss: 0.2529\n",
      "Epoch 16/20\n",
      "\u001b[1m1375/1375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9209 - loss: 0.2313 - val_accuracy: 0.9098 - val_loss: 0.2514\n",
      "Epoch 17/20\n",
      "\u001b[1m1375/1375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9217 - loss: 0.2286 - val_accuracy: 0.9108 - val_loss: 0.2500\n",
      "Epoch 18/20\n",
      "\u001b[1m1375/1375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9223 - loss: 0.2261 - val_accuracy: 0.9113 - val_loss: 0.2483\n",
      "Epoch 19/20\n",
      "\u001b[1m1375/1375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9230 - loss: 0.2237 - val_accuracy: 0.9098 - val_loss: 0.2474\n",
      "Epoch 20/20\n",
      "\u001b[1m1375/1375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9234 - loss: 0.2214 - val_accuracy: 0.9103 - val_loss: 0.2460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28,28]))\n",
    "for n_hidden in (300,100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation='selu'))\n",
    "model_A.add(keras.layers.Dense(8, activation='softmax'))\n",
    "\n",
    "model_A.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=0.001),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
    "                      validation_data=(X_valid_A, y_valid_A))\n",
    "\n",
    "model_A.save('model_A.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.2073 - loss: 2.3675 - val_accuracy: 0.5892 - val_loss: 1.2368\n",
      "Epoch 2/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6385 - loss: 1.0541 - val_accuracy: 0.7353 - val_loss: 0.8227\n",
      "Epoch 3/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8465 - loss: 0.6783 - val_accuracy: 0.8428 - val_loss: 0.5997\n",
      "Epoch 4/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9154 - loss: 0.4828 - val_accuracy: 0.9016 - val_loss: 0.4689\n",
      "Epoch 5/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9850 - loss: 0.3688 - val_accuracy: 0.9432 - val_loss: 0.3850\n",
      "Epoch 6/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9900 - loss: 0.2961 - val_accuracy: 0.9655 - val_loss: 0.3274\n",
      "Epoch 7/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.2467 - val_accuracy: 0.9736 - val_loss: 0.2857\n",
      "Epoch 8/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.2108 - val_accuracy: 0.9767 - val_loss: 0.2541\n",
      "Epoch 9/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.1837 - val_accuracy: 0.9777 - val_loss: 0.2295\n",
      "Epoch 10/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.1629 - val_accuracy: 0.9797 - val_loss: 0.2099\n",
      "Epoch 11/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.1463 - val_accuracy: 0.9817 - val_loss: 0.1937\n",
      "Epoch 12/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.1327 - val_accuracy: 0.9828 - val_loss: 0.1803\n",
      "Epoch 13/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.1213 - val_accuracy: 0.9828 - val_loss: 0.1689\n",
      "Epoch 14/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.1116 - val_accuracy: 0.9838 - val_loss: 0.1592\n",
      "Epoch 15/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.1034 - val_accuracy: 0.9838 - val_loss: 0.1508\n",
      "Epoch 16/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0962 - val_accuracy: 0.9838 - val_loss: 0.1435\n",
      "Epoch 17/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0900 - val_accuracy: 0.9838 - val_loss: 0.1371\n",
      "Epoch 18/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0846 - val_accuracy: 0.9838 - val_loss: 0.1314\n",
      "Epoch 19/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0797 - val_accuracy: 0.9838 - val_loss: 0.1263\n",
      "Epoch 20/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0753 - val_accuracy: 0.9838 - val_loss: 0.1218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model_B = keras.models.Sequential()\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28,28]))\n",
    "for n_hidden in (300,100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation='selu'))\n",
    "model_B.add(keras.layers.Dense(8, activation='softmax'))\n",
    "\n",
    "model_B.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=0.001),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "history = model_B.fit(X_train_B, y_train_B, epochs=20,\n",
    "                      validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "model_B.save('model_B.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.9248 - loss: 0.2101 - val_accuracy: 0.9686 - val_loss: 0.1647\n",
      "Epoch 2/5\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9805 - loss: 0.1408 - val_accuracy: 0.9878 - val_loss: 0.1268\n",
      "Epoch 3/5\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9854 - loss: 0.1077 - val_accuracy: 0.9909 - val_loss: 0.1053\n",
      "Epoch 4/5\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9904 - loss: 0.0884 - val_accuracy: 0.9939 - val_loss: 0.0914\n",
      "Epoch 5/5\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9931 - loss: 0.0759 - val_accuracy: 0.9949 - val_loss: 0.0817\n"
     ]
    }
   ],
   "source": [
    "model_A = keras.models.load_model('model_A.h5')\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B_on_A.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=5,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster Optimizers\n",
    "\n",
    "# Momentum Optimization - Use when facing high variance in gradients; helps accelerate SGD in the relevant direction \n",
    "# and dampens oscillations, making it effective in handling sparse gradients.\n",
    "momentum_optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n",
    "\n",
    "# Nesterov Accelerated Gradient - Best for problems where gradients exhibit momentum-like properties but require \n",
    "# more accurate, corrective steps; often used for computer vision tasks due to better convergence.\n",
    "nesterov_optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
    "\n",
    "# AdaGrad - Ideal for sparse data (like text or recommendation systems) as it adapts the learning rate for each \n",
    "# parameter individually, making frequent updates smaller and rare updates larger.\n",
    "ada_optimizer = keras.optimizers.Adagrad(learning_rate=0.001)\n",
    "\n",
    "# RMSProp - Works well for recurrent neural networks (RNNs) and other tasks with non-stationary objectives, \n",
    "# due to its ability to handle noisy gradients effectively.\n",
    "rms_prop_optimizer = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
    "\n",
    "# Adam Optimization - Generally recommended as a go-to optimizer for most deep learning tasks due to its adaptive \n",
    "# learning rate, making it highly effective in handling sparse gradients and requiring minimal tuning.\n",
    "adam_optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "# Adamax Optimization - A variant of Adam that is useful when gradients are unbounded or have a large range; \n",
    "# often chosen for models with non-smooth loss surfaces.\n",
    "adamax_optimizer = keras.optimizers.Adamax(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "# Nadam Optimization - Effective for sequences or time-series data as it combines Nesterov momentum with Adam, \n",
    "# often providing faster convergence in RNNs or LSTMs.\n",
    "nadam_optimizer = keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:33: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7927 - loss: 0.6022 - val_accuracy: 0.8598 - val_loss: 0.4061\n",
      "Epoch 2/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8625 - loss: 0.3833 - val_accuracy: 0.8698 - val_loss: 0.3743\n",
      "Epoch 3/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8762 - loss: 0.3456 - val_accuracy: 0.8730 - val_loss: 0.3576\n",
      "Epoch 4/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8846 - loss: 0.3209 - val_accuracy: 0.8766 - val_loss: 0.3479\n",
      "Epoch 5/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8919 - loss: 0.3022 - val_accuracy: 0.8788 - val_loss: 0.3417\n",
      "Epoch 6/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8976 - loss: 0.2868 - val_accuracy: 0.8824 - val_loss: 0.3370\n",
      "Epoch 7/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9027 - loss: 0.2733 - val_accuracy: 0.8832 - val_loss: 0.3341\n",
      "Epoch 8/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9070 - loss: 0.2615 - val_accuracy: 0.8850 - val_loss: 0.3327\n",
      "Epoch 9/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9105 - loss: 0.2509 - val_accuracy: 0.8856 - val_loss: 0.3320\n",
      "Epoch 10/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9141 - loss: 0.2411 - val_accuracy: 0.8856 - val_loss: 0.3313\n",
      "Epoch 11/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9178 - loss: 0.2320 - val_accuracy: 0.8858 - val_loss: 0.3316\n",
      "Epoch 12/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9207 - loss: 0.2235 - val_accuracy: 0.8864 - val_loss: 0.3325\n",
      "Epoch 13/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9243 - loss: 0.2154 - val_accuracy: 0.8862 - val_loss: 0.3339\n",
      "Epoch 14/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9277 - loss: 0.2076 - val_accuracy: 0.8864 - val_loss: 0.3356\n",
      "Epoch 15/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9305 - loss: 0.2001 - val_accuracy: 0.8862 - val_loss: 0.3380\n",
      "Epoch 16/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9331 - loss: 0.1930 - val_accuracy: 0.8878 - val_loss: 0.3399\n",
      "Epoch 17/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9358 - loss: 0.1862 - val_accuracy: 0.8872 - val_loss: 0.3429\n",
      "Epoch 18/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9388 - loss: 0.1795 - val_accuracy: 0.8872 - val_loss: 0.3459\n",
      "Epoch 19/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9415 - loss: 0.1732 - val_accuracy: 0.8870 - val_loss: 0.3494\n",
      "Epoch 20/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9440 - loss: 0.1671 - val_accuracy: 0.8854 - val_loss: 0.3535\n",
      "Epoch 21/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9460 - loss: 0.1611 - val_accuracy: 0.8856 - val_loss: 0.3578\n",
      "Epoch 22/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9483 - loss: 0.1554 - val_accuracy: 0.8868 - val_loss: 0.3616\n",
      "Epoch 23/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9506 - loss: 0.1499 - val_accuracy: 0.8870 - val_loss: 0.3654\n",
      "Epoch 24/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9523 - loss: 0.1446 - val_accuracy: 0.8870 - val_loss: 0.3709\n",
      "Epoch 25/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9541 - loss: 0.1394 - val_accuracy: 0.8874 - val_loss: 0.3757\n"
     ]
    }
   ],
   "source": [
    "#Learning Rate Scheduling - Power Scheduling\n",
    "#Set decay in the optimizer\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, decay=0.0001)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=25,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7568 - loss: 1.0098 - val_accuracy: 0.8056 - val_loss: 0.6997 - learning_rate: 0.0100\n",
      "Epoch 2/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8005 - loss: 0.6448 - val_accuracy: 0.8234 - val_loss: 0.6432 - learning_rate: 0.0089\n",
      "Epoch 3/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8193 - loss: 0.6193 - val_accuracy: 0.8206 - val_loss: 0.6284 - learning_rate: 0.0079\n",
      "Epoch 4/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8372 - loss: 0.5234 - val_accuracy: 0.8144 - val_loss: 0.6783 - learning_rate: 0.0071\n",
      "Epoch 5/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8370 - loss: 0.5407 - val_accuracy: 0.8328 - val_loss: 0.5373 - learning_rate: 0.0063\n",
      "Epoch 6/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8502 - loss: 0.4765 - val_accuracy: 0.8428 - val_loss: 0.6294 - learning_rate: 0.0056\n",
      "Epoch 7/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8625 - loss: 0.4406 - val_accuracy: 0.8546 - val_loss: 0.5187 - learning_rate: 0.0050\n",
      "Epoch 8/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8721 - loss: 0.4017 - val_accuracy: 0.8480 - val_loss: 0.6591 - learning_rate: 0.0045\n",
      "Epoch 9/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8759 - loss: 0.3726 - val_accuracy: 0.8638 - val_loss: 0.4648 - learning_rate: 0.0040\n",
      "Epoch 10/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8870 - loss: 0.3435 - val_accuracy: 0.8714 - val_loss: 0.4571 - learning_rate: 0.0035\n",
      "Epoch 11/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8915 - loss: 0.3239 - val_accuracy: 0.8672 - val_loss: 0.4799 - learning_rate: 0.0032\n",
      "Epoch 12/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8972 - loss: 0.2961 - val_accuracy: 0.8744 - val_loss: 0.5139 - learning_rate: 0.0028\n",
      "Epoch 13/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9065 - loss: 0.2762 - val_accuracy: 0.8770 - val_loss: 0.4855 - learning_rate: 0.0025\n",
      "Epoch 14/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9120 - loss: 0.2634 - val_accuracy: 0.8822 - val_loss: 0.5041 - learning_rate: 0.0022\n",
      "Epoch 15/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9171 - loss: 0.2426 - val_accuracy: 0.8820 - val_loss: 0.4946 - learning_rate: 0.0020\n",
      "Epoch 16/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9213 - loss: 0.2296 - val_accuracy: 0.8790 - val_loss: 0.5436 - learning_rate: 0.0018\n",
      "Epoch 17/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9280 - loss: 0.2145 - val_accuracy: 0.8810 - val_loss: 0.5386 - learning_rate: 0.0016\n",
      "Epoch 18/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9339 - loss: 0.1956 - val_accuracy: 0.8806 - val_loss: 0.5200 - learning_rate: 0.0014\n",
      "Epoch 19/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9365 - loss: 0.1882 - val_accuracy: 0.8864 - val_loss: 0.5514 - learning_rate: 0.0013\n",
      "Epoch 20/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9431 - loss: 0.1758 - val_accuracy: 0.8828 - val_loss: 0.5589 - learning_rate: 0.0011\n",
      "Epoch 21/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9466 - loss: 0.1664 - val_accuracy: 0.8850 - val_loss: 0.5601 - learning_rate: 0.0010\n",
      "Epoch 22/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9488 - loss: 0.1580 - val_accuracy: 0.8858 - val_loss: 0.5555 - learning_rate: 8.9125e-04\n",
      "Epoch 23/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9505 - loss: 0.1485 - val_accuracy: 0.8864 - val_loss: 0.5857 - learning_rate: 7.9433e-04\n",
      "Epoch 24/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9548 - loss: 0.1396 - val_accuracy: 0.8880 - val_loss: 0.6034 - learning_rate: 7.0795e-04\n",
      "Epoch 25/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9580 - loss: 0.1334 - val_accuracy: 0.8888 - val_loss: 0.6536 - learning_rate: 6.3096e-04\n"
     ]
    }
   ],
   "source": [
    "#Learning Rate Scheduling - Exponential Scheduling\n",
    "\n",
    "def exponential_decay_fn(epoch):\n",
    "    return 0.01*0.1**(epoch/20)\n",
    "\n",
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0*0.1**(epoch/20)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='nadam', metrics=[\"accuracy\"])\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=25,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.7528 - loss: 0.9996 - val_accuracy: 0.7646 - val_loss: 0.8388 - learning_rate: 0.0100\n",
      "Epoch 2/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7852 - loss: 0.7074 - val_accuracy: 0.7524 - val_loss: 0.8950 - learning_rate: 0.0100\n",
      "Epoch 3/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7663 - loss: 0.8088 - val_accuracy: 0.7250 - val_loss: 0.9168 - learning_rate: 0.0100\n",
      "Epoch 4/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7473 - loss: 0.8821 - val_accuracy: 0.8278 - val_loss: 0.6590 - learning_rate: 0.0100\n",
      "Epoch 5/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7726 - loss: 0.7713 - val_accuracy: 0.7334 - val_loss: 0.8751 - learning_rate: 0.0100\n",
      "Epoch 6/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7802 - loss: 0.6913 - val_accuracy: 0.8246 - val_loss: 0.6391 - learning_rate: 0.0050\n",
      "Epoch 7/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8388 - loss: 0.5143 - val_accuracy: 0.8566 - val_loss: 0.5731 - learning_rate: 0.0050\n",
      "Epoch 8/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8466 - loss: 0.4759 - val_accuracy: 0.8514 - val_loss: 0.6090 - learning_rate: 0.0050\n",
      "Epoch 9/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8543 - loss: 0.4511 - val_accuracy: 0.8426 - val_loss: 0.6566 - learning_rate: 0.0050\n",
      "Epoch 10/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8530 - loss: 0.4499 - val_accuracy: 0.8394 - val_loss: 0.6062 - learning_rate: 0.0050\n",
      "Epoch 11/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8631 - loss: 0.4267 - val_accuracy: 0.8498 - val_loss: 0.6765 - learning_rate: 0.0050\n",
      "Epoch 12/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8676 - loss: 0.4335 - val_accuracy: 0.8622 - val_loss: 0.6779 - learning_rate: 0.0050\n",
      "Epoch 13/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8720 - loss: 0.4202 - val_accuracy: 0.8696 - val_loss: 0.6986 - learning_rate: 0.0050\n",
      "Epoch 14/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8741 - loss: 0.4222 - val_accuracy: 0.8664 - val_loss: 0.6857 - learning_rate: 0.0050\n",
      "Epoch 15/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8768 - loss: 0.3951 - val_accuracy: 0.8666 - val_loss: 0.6054 - learning_rate: 0.0050\n",
      "Epoch 16/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8976 - loss: 0.3069 - val_accuracy: 0.8816 - val_loss: 0.4902 - learning_rate: 0.0010\n",
      "Epoch 17/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9085 - loss: 0.2625 - val_accuracy: 0.8832 - val_loss: 0.5116 - learning_rate: 0.0010\n",
      "Epoch 18/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9129 - loss: 0.2496 - val_accuracy: 0.8824 - val_loss: 0.5158 - learning_rate: 0.0010\n",
      "Epoch 19/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9181 - loss: 0.2357 - val_accuracy: 0.8830 - val_loss: 0.5493 - learning_rate: 0.0010\n",
      "Epoch 20/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9222 - loss: 0.2266 - val_accuracy: 0.8828 - val_loss: 0.6059 - learning_rate: 0.0010\n",
      "Epoch 21/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9258 - loss: 0.2162 - val_accuracy: 0.8864 - val_loss: 0.5734 - learning_rate: 0.0010\n",
      "Epoch 22/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9280 - loss: 0.2089 - val_accuracy: 0.8860 - val_loss: 0.5702 - learning_rate: 0.0010\n",
      "Epoch 23/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9323 - loss: 0.2004 - val_accuracy: 0.8854 - val_loss: 0.5940 - learning_rate: 0.0010\n",
      "Epoch 24/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9347 - loss: 0.1958 - val_accuracy: 0.8844 - val_loss: 0.6385 - learning_rate: 0.0010\n",
      "Epoch 25/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9369 - loss: 0.1929 - val_accuracy: 0.8832 - val_loss: 0.6215 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "#Learning Rate Scheduling - Piecewise Scheduling\n",
    "\n",
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001\n",
    "\n",
    "def piecewise_constant(boundaries, values):\n",
    "    boundaries = np.array([0] + boundaries)\n",
    "    values = np.array(values)\n",
    "    def piecewise_constant_fn(epoch):\n",
    "        return values[np.argmax(boundaries > epoch) - 1]\n",
    "    return piecewise_constant_fn\n",
    "\n",
    "piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='nadam', metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=25,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7753 - loss: 0.7166 - val_accuracy: 0.8430 - val_loss: 0.5025 - learning_rate: 0.0200\n",
      "Epoch 2/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8342 - loss: 0.5151 - val_accuracy: 0.8472 - val_loss: 0.5485 - learning_rate: 0.0200\n",
      "Epoch 3/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8416 - loss: 0.5107 - val_accuracy: 0.8374 - val_loss: 0.6376 - learning_rate: 0.0200\n",
      "Epoch 4/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8491 - loss: 0.5226 - val_accuracy: 0.8492 - val_loss: 0.5953 - learning_rate: 0.0200\n",
      "Epoch 5/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8429 - loss: 0.5669 - val_accuracy: 0.8490 - val_loss: 0.5764 - learning_rate: 0.0200\n",
      "Epoch 6/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8459 - loss: 0.5683 - val_accuracy: 0.8498 - val_loss: 0.7544 - learning_rate: 0.0200\n",
      "Epoch 7/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8791 - loss: 0.3667 - val_accuracy: 0.8774 - val_loss: 0.4312 - learning_rate: 0.0100\n",
      "Epoch 8/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8996 - loss: 0.2882 - val_accuracy: 0.8794 - val_loss: 0.4525 - learning_rate: 0.0100\n",
      "Epoch 9/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9089 - loss: 0.2571 - val_accuracy: 0.8782 - val_loss: 0.4577 - learning_rate: 0.0100\n",
      "Epoch 10/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9166 - loss: 0.2344 - val_accuracy: 0.8824 - val_loss: 0.4641 - learning_rate: 0.0100\n",
      "Epoch 11/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9232 - loss: 0.2182 - val_accuracy: 0.8776 - val_loss: 0.4912 - learning_rate: 0.0100\n",
      "Epoch 12/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9271 - loss: 0.2035 - val_accuracy: 0.8812 - val_loss: 0.5091 - learning_rate: 0.0100\n",
      "Epoch 13/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9360 - loss: 0.1695 - val_accuracy: 0.8828 - val_loss: 0.4693 - learning_rate: 0.0050\n",
      "Epoch 14/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9447 - loss: 0.1466 - val_accuracy: 0.8850 - val_loss: 0.4890 - learning_rate: 0.0050\n",
      "Epoch 15/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9500 - loss: 0.1322 - val_accuracy: 0.8834 - val_loss: 0.5105 - learning_rate: 0.0050\n",
      "Epoch 16/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9545 - loss: 0.1211 - val_accuracy: 0.8854 - val_loss: 0.5328 - learning_rate: 0.0050\n",
      "Epoch 17/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9584 - loss: 0.1115 - val_accuracy: 0.8856 - val_loss: 0.5557 - learning_rate: 0.0050\n",
      "Epoch 18/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9607 - loss: 0.1022 - val_accuracy: 0.8912 - val_loss: 0.5430 - learning_rate: 0.0025\n",
      "Epoch 19/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9658 - loss: 0.0901 - val_accuracy: 0.8904 - val_loss: 0.5511 - learning_rate: 0.0025\n",
      "Epoch 20/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9695 - loss: 0.0831 - val_accuracy: 0.8918 - val_loss: 0.5602 - learning_rate: 0.0025\n",
      "Epoch 21/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9714 - loss: 0.0776 - val_accuracy: 0.8914 - val_loss: 0.5713 - learning_rate: 0.0025\n",
      "Epoch 22/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9735 - loss: 0.0730 - val_accuracy: 0.8902 - val_loss: 0.5821 - learning_rate: 0.0025\n",
      "Epoch 23/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9755 - loss: 0.0682 - val_accuracy: 0.8930 - val_loss: 0.5744 - learning_rate: 0.0012\n",
      "Epoch 24/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9777 - loss: 0.0634 - val_accuracy: 0.8918 - val_loss: 0.5789 - learning_rate: 0.0012\n",
      "Epoch 25/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9789 - loss: 0.0605 - val_accuracy: 0.8920 - val_loss: 0.5840 - learning_rate: 0.0012\n"
     ]
    }
   ],
   "source": [
    "#Learning Rate Scheduling - Performance Scheduling\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.02, momentum=0.9)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 25\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7914 - loss: 0.6044 - val_accuracy: 0.8618 - val_loss: 0.4014\n",
      "Epoch 2/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8629 - loss: 0.3843 - val_accuracy: 0.8692 - val_loss: 0.3745\n",
      "Epoch 3/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8755 - loss: 0.3498 - val_accuracy: 0.8732 - val_loss: 0.3612\n",
      "Epoch 4/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8834 - loss: 0.3275 - val_accuracy: 0.8754 - val_loss: 0.3529\n",
      "Epoch 5/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8896 - loss: 0.3109 - val_accuracy: 0.8778 - val_loss: 0.3466\n",
      "Epoch 6/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8948 - loss: 0.2977 - val_accuracy: 0.8764 - val_loss: 0.3422\n",
      "Epoch 7/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8979 - loss: 0.2867 - val_accuracy: 0.8782 - val_loss: 0.3387\n",
      "Epoch 8/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9008 - loss: 0.2775 - val_accuracy: 0.8782 - val_loss: 0.3360\n",
      "Epoch 9/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9039 - loss: 0.2696 - val_accuracy: 0.8796 - val_loss: 0.3338\n",
      "Epoch 10/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9065 - loss: 0.2627 - val_accuracy: 0.8810 - val_loss: 0.3319\n",
      "Epoch 11/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9086 - loss: 0.2568 - val_accuracy: 0.8818 - val_loss: 0.3304\n",
      "Epoch 12/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9107 - loss: 0.2516 - val_accuracy: 0.8822 - val_loss: 0.3291\n",
      "Epoch 13/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9124 - loss: 0.2470 - val_accuracy: 0.8836 - val_loss: 0.3280\n",
      "Epoch 14/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9139 - loss: 0.2429 - val_accuracy: 0.8850 - val_loss: 0.3270\n",
      "Epoch 15/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9154 - loss: 0.2394 - val_accuracy: 0.8862 - val_loss: 0.3262\n",
      "Epoch 16/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9169 - loss: 0.2362 - val_accuracy: 0.8858 - val_loss: 0.3254\n",
      "Epoch 17/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9180 - loss: 0.2334 - val_accuracy: 0.8856 - val_loss: 0.3248\n",
      "Epoch 18/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9188 - loss: 0.2309 - val_accuracy: 0.8862 - val_loss: 0.3243\n",
      "Epoch 19/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9201 - loss: 0.2286 - val_accuracy: 0.8856 - val_loss: 0.3238\n",
      "Epoch 20/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9206 - loss: 0.2266 - val_accuracy: 0.8862 - val_loss: 0.3233\n",
      "Epoch 21/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9214 - loss: 0.2249 - val_accuracy: 0.8866 - val_loss: 0.3229\n",
      "Epoch 22/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9220 - loss: 0.2233 - val_accuracy: 0.8868 - val_loss: 0.3226\n",
      "Epoch 23/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9225 - loss: 0.2219 - val_accuracy: 0.8866 - val_loss: 0.3222\n",
      "Epoch 24/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9228 - loss: 0.2206 - val_accuracy: 0.8872 - val_loss: 0.3219\n",
      "Epoch 25/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9231 - loss: 0.2195 - val_accuracy: 0.8880 - val_loss: 0.3217\n"
     ]
    }
   ],
   "source": [
    "#Learning Rate Scheduling - tf.keras Schedulers\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
    "optimizer = keras.optimizers.SGD(learning_rate)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "n_epochs = 25\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7946 - loss: 3.2512 - val_accuracy: 0.8390 - val_loss: 0.7075\n",
      "Epoch 2/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8220 - loss: 0.7295 - val_accuracy: 0.8438 - val_loss: 0.6838\n",
      "Epoch 3/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8292 - loss: 0.7047 - val_accuracy: 0.8482 - val_loss: 0.6705\n",
      "Epoch 4/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8333 - loss: 0.6971 - val_accuracy: 0.8518 - val_loss: 0.6644\n",
      "Epoch 5/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8341 - loss: 0.6932 - val_accuracy: 0.8540 - val_loss: 0.6622\n",
      "Epoch 6/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8363 - loss: 0.6900 - val_accuracy: 0.8522 - val_loss: 0.6605\n",
      "Epoch 7/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8369 - loss: 0.6872 - val_accuracy: 0.8536 - val_loss: 0.6602\n",
      "Epoch 8/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8368 - loss: 0.6852 - val_accuracy: 0.8528 - val_loss: 0.6593\n",
      "Epoch 9/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8370 - loss: 0.6837 - val_accuracy: 0.8536 - val_loss: 0.6582\n",
      "Epoch 10/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8372 - loss: 0.6826 - val_accuracy: 0.8526 - val_loss: 0.6584\n"
     ]
    }
   ],
   "source": [
    "#Avoiding Overfitting Through L1 and L2 Regularization \n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(300, activation='elu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(100, activation='elu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(10, activation='softmax',\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "])\n",
    "\n",
    "# or l1(0.1) for ℓ1 regularization with a factor of 0.1\n",
    "# or l1_l2(0.1, 0.01) for both l1 and l2 regularization, with factors 0.1 and 0.01 respectively\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='nadam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=10,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7966 - loss: 3.3476 - val_accuracy: 0.8412 - val_loss: 0.7085\n",
      "Epoch 2/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - accuracy: 0.8228 - loss: 0.7309 - val_accuracy: 0.8444 - val_loss: 0.6870\n",
      "Epoch 3/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8311 - loss: 0.7051 - val_accuracy: 0.8458 - val_loss: 0.6778\n",
      "Epoch 4/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8362 - loss: 0.6954 - val_accuracy: 0.8482 - val_loss: 0.6722\n",
      "Epoch 5/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8389 - loss: 0.6900 - val_accuracy: 0.8522 - val_loss: 0.6668\n",
      "Epoch 6/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8399 - loss: 0.6863 - val_accuracy: 0.8490 - val_loss: 0.6636\n",
      "Epoch 7/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8399 - loss: 0.6839 - val_accuracy: 0.8496 - val_loss: 0.6624\n",
      "Epoch 8/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8402 - loss: 0.6827 - val_accuracy: 0.8496 - val_loss: 0.6605\n",
      "Epoch 9/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8400 - loss: 0.6815 - val_accuracy: 0.8494 - val_loss: 0.6599\n",
      "Epoch 10/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8406 - loss: 0.6804 - val_accuracy: 0.8496 - val_loss: 0.6584\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                           activation='elu',\n",
    "                           kernel_initializer='he_normal',\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train_scaled, y_train, epochs=10,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7614 - loss: 0.7497 - val_accuracy: 0.8662 - val_loss: 0.3672\n",
      "Epoch 2/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8432 - loss: 0.4317 - val_accuracy: 0.8698 - val_loss: 0.3567\n",
      "Epoch 3/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8535 - loss: 0.4014 - val_accuracy: 0.8760 - val_loss: 0.3362\n",
      "Epoch 4/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8612 - loss: 0.3769 - val_accuracy: 0.8752 - val_loss: 0.3313\n",
      "Epoch 5/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8636 - loss: 0.3715 - val_accuracy: 0.8840 - val_loss: 0.3189\n",
      "Epoch 6/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8688 - loss: 0.3503 - val_accuracy: 0.8814 - val_loss: 0.3230\n",
      "Epoch 7/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8715 - loss: 0.3419 - val_accuracy: 0.8852 - val_loss: 0.3231\n",
      "Epoch 8/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8771 - loss: 0.3340 - val_accuracy: 0.8864 - val_loss: 0.3191\n",
      "Epoch 9/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.8807 - loss: 0.3255 - val_accuracy: 0.8820 - val_loss: 0.3236\n",
      "Epoch 10/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.8823 - loss: 0.3185 - val_accuracy: 0.8868 - val_loss: 0.3136\n"
     ]
    }
   ],
   "source": [
    "#Dropout Regularization\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train_scaled, y_train, epochs=10,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.8048 - loss: 0.5722 - val_accuracy: 0.8706 - val_loss: 0.3629\n",
      "Epoch 2/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8665 - loss: 0.3678 - val_accuracy: 0.8712 - val_loss: 0.3530\n",
      "Epoch 3/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8785 - loss: 0.3335 - val_accuracy: 0.8744 - val_loss: 0.3541\n",
      "Epoch 4/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8831 - loss: 0.3195 - val_accuracy: 0.8758 - val_loss: 0.3560\n",
      "Epoch 5/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8845 - loss: 0.3121 - val_accuracy: 0.8744 - val_loss: 0.3512\n",
      "Epoch 6/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8876 - loss: 0.3058 - val_accuracy: 0.8758 - val_loss: 0.3521\n",
      "Epoch 7/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8861 - loss: 0.3046 - val_accuracy: 0.8772 - val_loss: 0.3440\n",
      "Epoch 8/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8891 - loss: 0.3019 - val_accuracy: 0.8730 - val_loss: 0.3479\n",
      "Epoch 9/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8909 - loss: 0.2952 - val_accuracy: 0.8770 - val_loss: 0.3547\n",
      "Epoch 10/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8901 - loss: 0.2961 - val_accuracy: 0.8776 - val_loss: 0.3436\n"
     ]
    }
   ],
   "source": [
    "MaxNormDense = partial(keras.layers.Dense,\n",
    "                       activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       kernel_constraint=keras.constraints.max_norm(1.))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    MaxNormDense(300),\n",
    "    MaxNormDense(100),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train_scaled, y_train, epochs=10,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
